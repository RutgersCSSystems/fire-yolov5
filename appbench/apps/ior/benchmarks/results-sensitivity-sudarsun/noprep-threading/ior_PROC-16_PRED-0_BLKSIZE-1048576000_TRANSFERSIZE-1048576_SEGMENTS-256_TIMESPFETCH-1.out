fclose PID:16688 fd:3
fclose PID:16688 fd:4
fclose PID:16688 fd:4
fclose PID:16688 fd:3
available: 2 nodes (0-1)
fclose PID:16688 fd:4
node 0 cpus:fclose PID:16688 fd:4
 0 1 2 3 4 5 6 7 16 17 18 19 20 21 22 23
node 0 size: 64178 MB
node 0 free: 63898 MB
fclose PID:16688 fd:4
node 1 cpus:fclose PID:16688 fd:4
 8 9 10 11 12 13 14 15 24 25 26 27 28 29 30 31
node 1 size: 64487 MB
node 1 free: 64299 MB
fclose PID:16688 fd:4
fclose PID:16688 fd:4
node distances:
node   0   1 
  0:  10  21 
  1:  21  10 
READAHEAD_TIME:0.000000 us
MaxRSS= 4000 KB, SharedMem= 0 KB, HardPageFault= 10
fclose PID:16697 fd:14
fclose PID:16705 fd:12
fclose PID:16706 fd:12
fclose PID:16711 fd:12
fclose PID:16696 fd:15
fclose PID:16698 fd:12
fclose PID:16699 fd:12
fclose PID:16700 fd:12
fclose PID:16701 fd:12
fclose PID:16702 fd:12
fclose PID:16703 fd:12
fclose PID:16704 fd:12
fclose PID:16707 fd:12
fclose PID:16708 fd:12
fclose PID:16709 fd:12
fclose PID:16710 fd:12
IOR-3.4.0+dev: MPI Coordinated Test of Parallel I/O
Began               : Mon Mar 15 18:12:09 2021
Command line        : ior -e -o test_outfile_ior -v -b 1048576000 -t 1048576 -s 256 -C -F -k
Machine             : Linux c220g1.sudarsun4.fsperfatscale-pg0.wisc.cloudlab.us
Start time skew across all tasks: 0.00 sec
TestID              : 0
StartTime           : Mon Mar 15 18:12:09 2021
Path                : test_outfile_ior.00000000
FS                  : 439.1 GiB   Used FS: 0.9%   Inodes: 28.0 Mi   Used Inodes: 0.6%
Participating tasks : 16
Using reorderTasks '-C' (useful to avoid read cache in client)

Options: 
api                 : POSIX
apiVersion          : 
test filename       : test_outfile_ior
access              : file-per-process
type                : independent
segments            : 256
ordering in a file  : sequential
ordering inter file : constant task offset
task offset         : 1
nodes               : 1
tasks               : 16
clients per node    : 16
repetitions         : 1
xfersize            : 1 MiB
blocksize           : 1000 MiB
aggregate filesize  : 3.91 TiB
verbose             : 1

Results: 

access    bw(MiB/s)  IOPS       Latency(s)  block(KiB) xfer(KiB)  open(s)    wr/rd(s)   close(s)   total(s)   iter
------    ---------  ----       ----------  ---------- ---------  --------   --------   --------   --------   ----
Commencing write performance test: Mon Mar 15 18:12:09 2021
WARNING: Task 5, partial write(), 950272 of 1048576 bytes at offset 27621588992
ERROR: write(12, 0x7fd95b4ab000, 98304) failed, (aiori-POSIX.c:563)
WARNING: Task 13, partial write(), 290816 of 1048576 bytes at offset 27658289152
ERROR: write(12, 0x7f93cdd0f000, 757760) failed, (aiori-POSIX.c:563)
application called MPI_Abort(MPI_COMM_WORLD, -1) - process 5
application called MPI_Abort(MPI_COMM_WORLD, -1) - process 13
WARNING: Task 1, partial write(), 167936 of 1048576 bytes at offset 27745320960
WARNING: Task 3, partial write(), 495616 of 1048576 bytes at offset 27801944064
ERROR: write(12, 0x7f8cb24ab000, 552960) failed, (aiori-POSIX.c:563)
WARNING: Task 11, partial write(), 1003520 of 1048576 bytes at offset 27624734720
ERROR: write(12, 0x7f9d841bc000, 45056) failed, (aiori-POSIX.c:563)
WARNING: Task 15, partial write(), 389120 of 1048576 bytes at offset 27655143424
ERROR: write(12, 0x7f3b97963000, 659456) failed, (aiori-POSIX.c:563)
ERROR: write(14, 0x7f5db8a5e000, 880640) failed, (aiori-POSIX.c:563)
application called MPI_Abort(MPI_COMM_WORLD, -1) - process 1
application called MPI_Abort(MPI_COMM_WORLD, -1) - process 3
application called MPI_Abort(MPI_COMM_WORLD, -1) - process 11
application called MPI_Abort(MPI_COMM_WORLD, -1) - process 15
WARNING: Task 0, partial write(), 290816 of 1048576 bytes at offset 27916238848
ERROR: write(15, 0x7f601405b000, 757760) failed, (aiori-POSIX.c:563)
WARNING: Task 10, partial write(), 528384 of 1048576 bytes at offset 27789361152
ERROR: write(12, 0x7ff821a24000, 520192) failed, (aiori-POSIX.c:563)
WARNING: Task 14, partial write(), 1024000 of 1048576 bytes at offset 27729592320
ERROR: write(12, 0x7f8731944000, 24576) failed, (aiori-POSIX.c:563)
applicatio