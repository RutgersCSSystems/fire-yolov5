fclose PID:8921 fd:3
fclose PID:8921 fd:4
fclose PID:8921 fd:4
fclose PID:8921 fd:3
available: 2 nodes (0-1)
fclose PID:8921 fd:4
node 0 cpus:fclose PID:8921 fd:4
 0 1 2 3 4 5 6 7 16 17 18 19 20 21 22 23
node 0 size: 64178 MB
node 0 free: 63888 MB
fclose PID:8921 fd:4
node 1 cpus:fclose PID:8921 fd:4
 8 9 10 11 12 13 14 15 24 25 26 27 28 29 30 31
node 1 size: 64487 MB
node 1 free: 64301 MB
fclose PID:8921 fd:4
fclose PID:8921 fd:4
node distances:
node   0   1 
  0:  10  21 
  1:  21  10 
READAHEAD_TIME:0.000000 us
MaxRSS= 4044 KB, SharedMem= 0 KB, HardPageFault= 9
fclose PID:8936 fd:12
fclose PID:8935 fd:12
fclose PID:8931 fd:12
fclose PID:8932 fd:12
fclose PID:8929 fd:15
fclose PID:8930 fd:14
fclose PID:8933 fd:12
fclose PID:8934 fd:12
IOR-3.4.0+dev: MPI Coordinated Test of Parallel I/O
Began               : Tue Mar 16 15:29:30 2021
Command line        : ior -e -o test_outfile_ior -v -b 1048576000 -t 1048576 -s 256 -C -F -k
Machine             : Linux c220g1.sudarsun4.fsperfatscale-pg0.wisc.cloudlab.us
Start time skew across all tasks: 0.00 sec
TestID              : 0
StartTime           : Tue Mar 16 15:29:30 2021
Path                : test_outfile_ior.00000000
FS                  : 439.1 GiB   Used FS: 0.9%   Inodes: 28.0 Mi   Used Inodes: 0.6%
Participating tasks : 8
Using reorderTasks '-C' (useful to avoid read cache in client)

Options: 
api                 : POSIX
apiVersion          : 
test filename       : test_outfile_ior
access              : file-per-process
type                : independent
segments            : 256
ordering in a file  : sequential
ordering inter file : constant task offset
task offset         : 1
nodes               : 1
tasks               : 8
clients per node    : 8
repetitions         : 1
xfersize            : 1 MiB
blocksize           : 1000 MiB
aggregate filesize  : 1.95 TiB
verbose             : 1

Results: 

access    bw(MiB/s)  IOPS       Latency(s)  block(KiB) xfer(KiB)  open(s)    wr/rd(s)   close(s)   total(s)   iter
------    ---------  ----       ----------  ---------- ---------  --------   --------   --------   --------   ----
Commencing write performance test: Tue Mar 16 15:29:30 2021
WARNING: Task 4, partial write(), 929792 of 1048576 bytes at offset 55320772608
ERROR: write(12, 0x7f977e13c000, 118784) failed, (aiori-POSIX.c:563)
application called MPI_Abort(MPI_COMM_WORLD, -1) - process 4
WARNING: Task 2, partial write(), 831488 of 1048576 bytes at offset 55451844608
ERROR: write(12, 0x7f0e7dd7f000, 217088) failed, (aiori-POSIX.c:563)
WARNING: Task 1, partial write(), 221184 of 1048576 bytes at offset 55378444288
application called MPI_Abort(MPI_COMM_WORLD, -1) - process 2
ERROR: write(14, 0x7f0a47c56000, 827392) failed, (aiori-POSIX.c:563)
application called MPI_Abort(MPI_COMM_WORLD, -1) - process 1
WARNING: Task 0, partial write(), 573440 of 1048576 bytes at offset 55450796032
ERROR: write(15, 0x7f2110511000, 475136) failed, (aiori-POSIX.c:563)
application called MPI_Abort(MPI_COMM_WORLD, -1) - process 0
WARNING: Task 6, partial write(), 1044480 of 1048576 bytes at offset 55469670400
ERROR: write(12, 0x7f9ce16a9000, 4096) failed, (aiori-POSIX.c:563)
application called MPI_Abort(MPI_COMM_WORLD, -1) - process 6
WARNING: Task 3, partial write(), 139264 of 1048576 bytes at offset 55354327040
ERROR: write(12, 0x7ff6f264e000, 909312) failed, (aiori-POSIX.c:563)
application called MPI_Abort(MPI_COMM_WORLD, -1) - process 3
fclose PID:8927 fd:4
fclose PID:8927 fd:5
fclose PID:8927 fd:5
fclose PID:8927 fd:4
READAHEAD_TIME:0.000000 us
MaxRSS= 7560 KB, SharedMem= 0 KB, HardPageFault= 4
Command exited with non-zero status 255
	Command being timed: "mpirun -np 8 ior -e -o test_outfile_ior -v -b 1048576000 -t 1048576 -s 256 -C -F -k"
	User time (seconds): 0.00
	System time (seconds): 0.00
	Percent of CPU this job got: 0%
	Elapsed (wall clock) time (h:mm:ss or m:ss): 16:30.25
	Average shared text size (kbytes): 0
	Average unshared data size (kbytes): 0
	Average stack size (kbytes): 0
	Average total size (kbytes): 0
	Maximum resident set size (kbytes): 7744
	Average resident set size (kbyte