+ sudo dmesg -c
[45836.907074] drop_caches: sh (15568): drop_caches: 3
[45838.316420] drop_caches: sh (15583): drop_caches: 3
[45843.123845] audit: type=1400 audit(1572444036.551:4280): apparmor="STATUS" operation="profile_replace" info="same as current profile, skipping" profile="unconfined" name="/usr/sbin/mysqld" pid=15653 comm="apparmor_parser"
[45844.014855] audit: type=1400 audit(1572444037.443:4281): apparmor="DENIED" operation="open" profile="/usr/sbin/mysqld" name="/proc/15671/status" pid=15671 comm="mysqld" requested_mask="r" denied_mask="r" fsuid=0 ouid=0
[45844.014860] audit: type=1400 audit(1572444037.443:4282): apparmor="DENIED" operation="open" profile="/usr/sbin/mysqld" name="/sys/devices/system/node/" pid=15671 comm="mysqld" requested_mask="r" denied_mask="r" fsuid=0 ouid=0
[45844.014889] audit: type=1400 audit(1572444037.443:4283): apparmor="DENIED" operation="open" profile="/usr/sbin/mysqld" name="/proc/15671/status" pid=15671 comm="mysqld" requested_mask="r" denied_mask="r" fsuid=0 ouid=0
[45844.205118] audit: type=1400 audit(1572444037.635:4284): apparmor="DENIED" operation="capable" profile="/usr/sbin/mysqld" pid=15671 comm="mysqld" capability=2  capname="dac_read_search"
[45844.575273] audit: type=1400 audit(1572444038.003:4285): apparmor="STATUS" operation="profile_replace" info="same as current profile, skipping" profile="unconfined" name="/usr/sbin/mysqld" pid=15685 comm="apparmor_parser"
[45845.270200] audit: type=1400 audit(1572444038.699:4286): apparmor="DENIED" operation="open" profile="/usr/sbin/mysqld" name="/proc/15761/status" pid=15761 comm="mysqld" requested_mask="r" denied_mask="r" fsuid=114 ouid=114
[45845.270208] audit: type=1400 audit(1572444038.699:4287): apparmor="DENIED" operation="open" profile="/usr/sbin/mysqld" name="/sys/devices/system/node/" pid=15761 comm="mysqld" requested_mask="r" denied_mask="r" fsuid=114 ouid=0
[45845.270231] audit: type=1400 audit(1572444038.699:4288): apparmor="DENIED" operation="open" profile="/usr/sbin/mysqld" name="/proc/15761/status" pid=15761 comm="mysqld" requested_mask="r" denied_mask="r" fsuid=114 ouid=114
[45877.029411] audit: type=1400 audit(1572444070.460:4289): apparmor="DENIED" operation="open" profile="/usr/sbin/mysqld" name="/proc/15930/status" pid=15930 comm="mysqld" requested_mask="r" denied_mask="r" fsuid=114 ouid=114
[45877.029429] audit: type=1400 audit(1572444070.460:4290): apparmor="DENIED" operation="open" profile="/usr/sbin/mysqld" name="/sys/devices/system/node/" pid=15930 comm="mysqld" requested_mask="r" denied_mask="r" fsuid=114 ouid=0
[45877.029513] audit: type=1400 audit(1572444070.460:4291): apparmor="DENIED" operation="open" profile="/usr/sbin/mysqld" name="/proc/15930/status" pid=15930 comm="mysqld" requested_mask="r" denied_mask="r" fsuid=114 ouid=114
[45891.128101] nvmemul: ioctl command: 0xc008aa02
[45891.128107] nvmemul: getpci bus_id 0xff device_id 0x14, function_id 0x0, offset 0x190, val 0x8186
[45891.128115] nvmemul: ioctl command: 0x8008aa01
[45891.128118] nvmemul: setpci bus_id=0xff device_id=0x14, function_id=0x0, val=0x8fff
[45891.128121] nvmemul: ioctl command: 0x8008aa01
[45891.128123] nvmemul: setpci bus_id=0xff device_id=0x14, function_id=0x1, val=0x8fff
[45891.128126] nvmemul: ioctl command: 0x8008aa01
[45891.128128] nvmemul: setpci bus_id=0xff device_id=0x17, function_id=0x0, val=0x8fff
[45891.128132] nvmemul: ioctl command: 0x8008aa01
[45891.128133] nvmemul: setpci bus_id=0xff device_id=0x17, function_id=0x1, val=0x8fff
[45891.128138] nvmemul: ioctl command: 0xc008aa02
[45891.128139] nvmemul: getpci bus_id 0xff device_id 0x14, function_id 0x0, offset 0x190, val 0x8fff
[45891.128143] nvmemul: ioctl command: 0x8008aa01
[45891.128144] nvmemul: setpci bus_id=0xff device_id=0x14, function_id=0x0, val=0x800f
[45891.128149] nvmemul: ioctl command: 0x8008aa01
[45891.128150] nvmemul: setpci bus_id=0xff device_id=0x14, function_id=0x1, val=0x800f
[45891.128154] nvmemul: ioctl command: 0x8008aa01
[45891.128166] nvmemul: setpci bus_id=0xff device_id=0x17, function_id=0x0, val=0x800f
[45891.128170] nvmemul: ioctl command: 0x8008aa01
[45891.128171] nvmemul: setpci bus_id=0xff device_id=0x17, function_id=0x1, val=0x800f
[45915.714031] audit: type=1400 audit(1572444109.145:4292): apparmor="DENIED" operation="open" profile="/usr/sbin/mysqld" name="/proc/16625/status" pid=16625 comm="mysqld" requested_mask="r" denied_mask="r" fsuid=114 ouid=114
[45915.716715] audit: type=1400 audit(1572444109.149:4293): apparmor="DENIED" operation="open" profile="/usr/sbin/mysqld" name="/sys/devices/system/node/" pid=16625 comm="mysqld" requested_mask="r" denied_mask="r" fsuid=114 ouid=0
[45915.722454] audit: type=1400 audit(1572444109.153:4294): apparmor="DENIED" operation="open" profile="/usr/sbin/mysqld" name="/proc/16625/status" pid=16625 comm="mysqld" requested_mask="r" denied_mask="r" fsuid=114 ouid=114
[45931.746276] perf: interrupt took too long (96730 > 94665), lowering kernel.perf_event_max_sample_rate to 2000
[45948.824687] nvmemul: ioctl command: 0xc008aa02
[45948.824809] nvmemul: getpci bus_id 0xff device_id 0x14, function_id 0x0, offset 0x190, val 0x800f
[45948.824993] nvmemul: ioctl command: 0x8008aa01
[45948.825024] nvmemul: setpci bus_id=0xff device_id=0x14, function_id=0x0, val=0x801e
[45948.825100] nvmemul: ioctl command: 0x8008aa01
[45948.825124] nvmemul: setpci bus_id=0xff device_id=0x14, function_id=0x1, val=0x801e
[45948.825147] nvmemul: ioctl command: 0x8008aa01
[45948.825155] nvmemul: setpci bus_id=0xff device_id=0x17, function_id=0x0, val=0x801e
[45948.825187] nvmemul: ioctl command: 0x8008aa01
[45948.825238] nvmemul: setpci bus_id=0xff device_id=0x17, function_id=0x1, val=0x801e
[45964.574267] audit: type=1400 audit(1572444158.002:4295): apparmor="DENIED" operation="open" profile="/usr/sbin/mysqld" name="/proc/16734/status" pid=16734 comm="mysqld" requested_mask="r" denied_mask="r" fsuid=114 ouid=114
[45964.575023] audit: type=1400 audit(1572444158.006:4296): apparmor="DENIED" operation="open" profile="/usr/sbin/mysqld" name="/sys/devices/system/node/" pid=16734 comm="mysqld" requested_mask="r" denied_mask="r" fsuid=114 ouid=0
[45964.576136] audit: type=1400 audit(1572444158.006:4297): apparmor="DENIED" operation="open" profile="/usr/sbin/mysqld" name="/proc/16734/status" pid=16734 comm="mysqld" requested_mask="r" denied_mask="r" fsuid=114 ouid=114
[45971.085032] nvmemul: ioctl command: 0xc008aa02
[45971.085072] nvmemul: getpci bus_id 0xff device_id 0x14, function_id 0x0, offset 0x190, val 0x801e
[45971.085249] nvmemul: ioctl command: 0x8008aa01
[45971.085318] nvmemul: setpci bus_id=0xff device_id=0x14, function_id=0x0, val=0x802d
[45971.085372] nvmemul: ioctl command: 0x8008aa01
[45971.085387] nvmemul: setpci bus_id=0xff device_id=0x14, function_id=0x1, val=0x802d
[45971.085425] nvmemul: ioctl command: 0x8008aa01
[45971.085464] nvmemul: setpci bus_id=0xff device_id=0x17, function_id=0x0, val=0x802d
[45971.085487] nvmemul: ioctl command: 0x8008aa01
[45971.085502] nvmemul: setpci bus_id=0xff device_id=0x17, function_id=0x1, val=0x802d
[45986.488841] nvmemul: ioctl command: 0xc008aa02
[45986.488893] nvmemul: getpci bus_id 0xff device_id 0x14, function_id 0x0, offset 0x190, val 0x802d
[45986.488920] nvmemul: ioctl command: 0x8008aa01
[45986.488932] nvmemul: setpci bus_id=0xff device_id=0x14, function_id=0x0, val=0x803c
[45986.488943] nvmemul: ioctl command: 0x8008aa01
[45986.488954] nvmemul: setpci bus_id=0xff device_id=0x14, function_id=0x1, val=0x803c
[45986.488958] nvmemul: ioctl command: 0x8008aa01
[45986.488963] nvmemul: setpci bus_id=0xff device_id=0x17, function_id=0x0, val=0x803c
[45986.488973] nvmemul: ioctl command: 0x8008aa01
[45986.488975] nvmemul: setpci bus_id=0xff device_id=0x17, function_id=0x1, val=0x803c
[45999.208180] nvmemul: ioctl command: 0xc008aa02
[45999.208231] nvmemul: getpci bus_id 0xff device_id 0x14, function_id 0x0, offset 0x190, val 0x803c
[45999.208278] nvmemul: ioctl command: 0x8008aa01
[45999.208293] nvmemul: setpci bus_id=0xff device_id=0x14, function_id=0x0, val=0x804b
[45999.208308] nvmemul: ioctl command: 0x8008aa01
[45999.208309] nvmemul: setpci bus_id=0xff device_id=0x14, function_id=0x1, val=0x804b
[45999.208323] nvmemul: ioctl command: 0x8008aa01
[45999.208325] nvmemul: setpci bus_id=0xff device_id=0x17, function_id=0x0, val=0x804b
[45999.208339] nvmemul: ioctl command: 0x8008aa01
[45999.208340] nvmemul: setpci bus_id=0xff device_id=0x17, function_id=0x1, val=0x804b
[45999.822580] audit: type=1400 audit(1572444193.255:4298): apparmor="DENIED" operation="open" profile="/usr/sbin/mysqld" name="/proc/16838/status" pid=16838 comm="mysqld" requested_mask="r" denied_mask="r" fsuid=114 ouid=114
[45999.823009] audit: type=1400 audit(1572444193.255:4299): apparmor="DENIED" operation="open" profile="/usr/sbin/mysqld" name="/sys/devices/system/node/" pid=16838 comm="mysqld" requested_mask="r" denied_mask="r" fsuid=114 ouid=0
[45999.823017] audit: type=1400 audit(1572444193.255:4300): apparmor="DENIED" operation="open" profile="/usr/sbin/mysqld" name="/proc/16838/status" pid=16838 comm="mysqld" requested_mask="r" denied_mask="r" fsuid=114 ouid=114
[46010.446090] nvmemul: ioctl command: 0xc008aa02
[46010.446104] nvmemul: getpci bus_id 0xff device_id 0x14, function_id 0x0, offset 0x190, val 0x804b
[46010.446143] nvmemul: ioctl command: 0x8008aa01
[46010.446158] nvmemul: setpci bus_id=0xff device_id=0x14, function_id=0x0, val=0x805a
[46010.446172] nvmemul: ioctl command: 0x8008aa01
[46010.446174] nvmemul: setpci bus_id=0xff device_id=0x14, function_id=0x1, val=0x805a
[46010.446188] nvmemul: ioctl command: 0x8008aa01
[46010.446189] nvmemul: setpci bus_id=0xff device_id=0x17, function_id=0x0, val=0x805a
[46010.446197] nvmemul: ioctl command: 0x8008aa01
[46010.446204] nvmemul: setpci bus_id=0xff device_id=0x17, function_id=0x1, val=0x805a
[46033.081632] audit: type=1400 audit(1572444226.515:4301): apparmor="DENIED" operation="open" profile="/usr/sbin/mysqld" name="/proc/17041/status" pid=17041 comm="mysqld" requested_mask="r" denied_mask="r" fsuid=114 ouid=114
[46033.081812] audit: type=1400 audit(1572444226.515:4302): apparmor="DENIED" operation="open" profile="/usr/sbin/mysqld" name="/sys/devices/system/node/" pid=17041 comm="mysqld" requested_mask="r" denied_mask="r" fsuid=114 ouid=0
[46033.082003] audit: type=1400 audit(1572444226.515:4303): apparmor="DENIED" operation="open" profile="/usr/sbin/mysqld" name="/proc/17041/status" pid=17041 comm="mysqld" requested_mask="r" denied_mask="r" fsuid=114 ouid=114
[46043.508046] nvmemul: ioctl command: 0xc008aa02
[46043.508054] nvmemul: getpci bus_id 0xff device_id 0x14, function_id 0x0, offset 0x190, val 0x805a
[46043.508078] nvmemul: ioctl command: 0x8008aa01
[46043.508083] nvmemul: setpci bus_id=0xff device_id=0x14, function_id=0x0, val=0x8fff
[46043.508087] nvmemul: ioctl command: 0x8008aa01
[46043.508088] nvmemul: setpci bus_id=0xff device_id=0x14, function_id=0x1, val=0x8fff
[46043.508092] nvmemul: ioctl command: 0x8008aa01
[46043.508093] nvmemul: setpci bus_id=0xff device_id=0x17, function_id=0x0, val=0x8fff
[46043.508098] nvmemul: ioctl command: 0x8008aa01
[46043.508099] nvmemul: setpci bus_id=0xff device_id=0x17, function_id=0x1, val=0x8fff
[46043.508103] nvmemul: ioctl command: 0xc008aa02
[46043.508105] nvmemul: getpci bus_id 0xff device_id 0x14, function_id 0x0, offset 0x190, val 0x8fff
[46043.508109] nvmemul: ioctl command: 0x8008aa01
[46043.508110] nvmemul: setpci bus_id=0xff device_id=0x14, function_id=0x0, val=0x800f
[46043.508114] nvmemul: ioctl command: 0x8008aa01
[46043.508116] nvmemul: setpci bus_id=0xff device_id=0x14, function_id=0x1, val=0x800f
[46043.508119] nvmemul: ioctl command: 0x8008aa01
[46043.508120] nvmemul: setpci bus_id=0xff device_id=0x17, function_id=0x0, val=0x800f
[46043.508124] nvmemul: ioctl command: 0x8008aa01
[46043.508125] nvmemul: setpci bus_id=0xff device_id=0x17, function_id=0x1, val=0x800f
[46075.737687] audit: type=1400 audit(1572444269.172:4304): apparmor="DENIED" operation="open" profile="/usr/sbin/mysqld" name="/proc/17162/status" pid=17162 comm="mysqld" requested_mask="r" denied_mask="r" fsuid=114 ouid=114
[46075.743095] audit: type=1400 audit(1572444269.176:4305): apparmor="DENIED" operation="open" profile="/usr/sbin/mysqld" name="/sys/devices/system/node/" pid=17162 comm="mysqld" requested_mask="r" denied_mask="r" fsuid=114 ouid=0
[46075.753622] audit: type=1400 audit(1572444269.188:4306): apparmor="DENIED" operation="open" profile="/usr/sbin/mysqld" name="/proc/17162/status" pid=17162 comm="mysqld" requested_mask="r" denied_mask="r" fsuid=114 ouid=114
[46090.330218] perf: interrupt took too long (125691 > 120912), lowering kernel.perf_event_max_sample_rate to 1500
[46096.322281] nvmemul: ioctl command: 0xc008aa02
[46096.322456] nvmemul: getpci bus_id 0xff device_id 0x14, function_id 0x0, offset 0x190, val 0x800f
[46096.322864] nvmemul: ioctl command: 0x8008aa01
[46096.322895] nvmemul: setpci bus_id=0xff device_id=0x14, function_id=0x0, val=0x801e
[46096.322949] nvmemul: ioctl command: 0x8008aa01
[46096.322964] nvmemul: setpci bus_id=0xff device_id=0x14, function_id=0x1, val=0x801e
[46096.323002] nvmemul: ioctl command: 0x8008aa01
[46096.323004] nvmemul: setpci bus_id=0xff device_id=0x17, function_id=0x0, val=0x801e
[46096.323380] nvmemul: ioctl command: 0x8008aa01
[46096.323387] nvmemul: setpci bus_id=0xff device_id=0x17, function_id=0x1, val=0x801e
[46118.857846] nvmemul: ioctl command: 0xc008aa02
[46118.857929] nvmemul: getpci bus_id 0xff device_id 0x14, function_id 0x0, offset 0x190, val 0x801e
[46118.858029] nvmemul: ioctl command: 0x8008aa01
[46118.858036] nvmemul: setpci bus_id=0xff device_id=0x14, function_id=0x0, val=0x802d
[46118.858044] nvmemul: ioctl command: 0x8008aa01
[46118.858052] nvmemul: setpci bus_id=0xff device_id=0x14, function_id=0x1, val=0x802d
[46118.858060] nvmemul: ioctl command: 0x8008aa01
[46118.858068] nvmemul: setpci bus_id=0xff device_id=0x17, function_id=0x0, val=0x802d
[46118.858097] nvmemul: ioctl command: 0x8008aa01
[46118.858098] nvmemul: setpci bus_id=0xff device_id=0x17, function_id=0x1, val=0x802d
[46120.118889] audit: type=1400 audit(1572444313.557:4307): apparmor="DENIED" operation="open" profile="/usr/sbin/mysqld" name="/proc/17266/status" pid=17266 comm="mysqld" requested_mask="r" denied_mask="r" fsuid=114 ouid=114
[46120.119114] audit: type=1400 audit(1572444313.557:4308): apparmor="DENIED" operation="open" profile="/usr/sbin/mysqld" name="/sys/devices/system/node/" pid=17266 comm="mysqld" requested_mask="r" denied_mask="r" fsuid=114 ouid=0
[46120.119275] audit: type=1400 audit(1572444313.557:4309): apparmor="DENIED" operation="open" profile="/usr/sbin/mysqld" name="/proc/17266/status" pid=17266 comm="mysqld" requested_mask="r" denied_mask="r" fsuid=114 ouid=114
[46134.261583] nvmemul: ioctl command: 0xc008aa02
[46134.261620] nvmemul: getpci bus_id 0xff device_id 0x14, function_id 0x0, offset 0x190, val 0x802d
[46134.261673] nvmemul: ioctl command: 0x8008aa01
[46134.261681] nvmemul: setpci bus_id=0xff device_id=0x14, function_id=0x0, val=0x803c
[46134.261697] nvmemul: ioctl command: 0x8008aa01
[46134.261698] nvmemul: setpci bus_id=0xff device_id=0x14, function_id=0x1, val=0x803c
[46134.261712] nvmemul: ioctl command: 0x8008aa01
[46134.261713] nvmemul: setpci bus_id=0xff device_id=0x17, function_id=0x0, val=0x803c
[46134.261727] nvmemul: ioctl command: 0x8008aa01
[46134.261728] nvmemul: setpci bus_id=0xff device_id=0x17, function_id=0x1, val=0x803c
[46146.828065] nvmemul: ioctl command: 0xc008aa02
[46146.828112] nvmemul: getpci bus_id 0xff device_id 0x14, function_id 0x0, offset 0x190, val 0x803c
[46146.828150] nvmemul: ioctl command: 0x8008aa01
[46146.828158] nvmemul: setpci bus_id=0xff device_id=0x14, function_id=0x0, val=0x804b
[46146.828173] nvmemul: ioctl command: 0x8008aa01
[46146.828181] nvmemul: setpci bus_id=0xff device_id=0x14, function_id=0x1, val=0x804b
[46146.828195] nvmemul: ioctl command: 0x8008aa01
[46146.828197] nvmemul: setpci bus_id=0xff device_id=0x17, function_id=0x0, val=0x804b
[46146.828211] nvmemul: ioctl command: 0x8008aa01
[46146.828219] nvmemul: setpci bus_id=0xff device_id=0x17, function_id=0x1, val=0x804b
[46154.683418] audit: type=1400 audit(1572444348.118:4310): apparmor="DENIED" operation="open" profile="/usr/sbin/mysqld" name="/proc/17375/status" pid=17375 comm="mysqld" requested_mask="r" denied_mask="r" fsuid=114 ouid=114
[46154.684479] audit: type=1400 audit(1572444348.118:4311): apparmor="DENIED" operation="open" profile="/usr/sbin/mysqld" name="/sys/devices/system/node/" pid=17375 comm="mysqld" requested_mask="r" denied_mask="r" fsuid=114 ouid=0
[46154.685595] audit: type=1400 audit(1572444348.118:4312): apparmor="DENIED" operation="open" profile="/usr/sbin/mysqld" name="/proc/17375/status" pid=17375 comm="mysqld" requested_mask="r" denied_mask="r" fsuid=114 ouid=114
[46157.880087] nvmemul: ioctl command: 0xc008aa02
[46157.880109] nvmemul: getpci bus_id 0xff device_id 0x14, function_id 0x0, offset 0x190, val 0x804b
[46157.880147] nvmemul: ioctl command: 0x8008aa01
[46157.880169] nvmemul: setpci bus_id=0xff device_id=0x14, function_id=0x0, val=0x805a
[46157.880184] nvmemul: ioctl command: 0x8008aa01
[46157.880186] nvmemul: setpci bus_id=0xff device_id=0x14, function_id=0x1, val=0x805a
[46157.880200] nvmemul: ioctl command: 0x8008aa01
[46157.880202] nvmemul: setpci bus_id=0xff device_id=0x17, function_id=0x0, val=0x805a
[46157.880216] nvmemul: ioctl command: 0x8008aa01
[46157.880230] nvmemul: setpci bus_id=0xff device_id=0x17, function_id=0x1, val=0x805a
[46173.811223] drop_caches: sh (17454): drop_caches: 3
[46175.236397] drop_caches: sh (17464): drop_caches: 3
+ /users/skannan/ssd/NVM/appbench/shared_libs/construct/reset
+ cd /users/skannan/ssd/NVM/appbench/apps/spark/spark-bench
+ stop-dfs.sh
Stopping namenodes on [localhost]
Stopping datanodes
Stopping secondary namenodes [localhost]
application termination...
+ stop-yarn.sh
Stopping nodemanagers
Stopping resourcemanager
+ rm -rf /users/skannan/ssd/NVM/appbench/apps/spark/hadoop-3.2.1/hdfs/datanode /users/skannan/ssd/NVM/appbench/apps/spark/hadoop-3.2.1/hdfs/namenode /users/skannan/ssd/NVM/appbench/apps/spark/hadoop-3.2.1/hdfs/tmp
+ hadoop namenode -format
WARNING: Use of this script to execute namenode is deprecated.
WARNING: Attempting to execute replacement "hdfs namenode" instead.

2019-10-30 08:06:34,771 INFO namenode.NameNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting NameNode
STARTUP_MSG:   host = node-1.16-04-kernel.fsperfatscale-pg0.wisc.cloudlab.us/127.0.0.1
STARTUP_MSG:   args = [-format]
STARTUP_MSG:   version = 3.2.1
STARTUP_MSG:   classpath = /users/skannan/ssd/NVM/appbench/apps/spark/hadoop-3.2.1/etc/hadoop:/users/skannan/ssd/NVM/appbench/apps/spark/hadoop-3.2.1/share/hadoop/common/lib/paranamer-2.3.jar:/users/skannan/ssd/NVM/appbench/apps/spark/hadoop-3.2.1/share/hadoop/common/lib/jsch-0.1.54.jar:/users/skannan/ssd/NVM/appbench/apps/spark/hadoop-3.2.1/share/hadoop/common/lib/hadoop-auth-3.2.1.jar:/users/skannan/ssd/NVM/appbench/apps/spark/hadoop-3.2.1/share/hadoop/common/lib/jersey-json-1.19.jar:/users/skannan/ssd/NVM/appbench/apps/spark/hadoop-3.2.1/share/hadoop/common/lib/jersey-core-1.19.jar:/users/skannan/ssd/NVM/appbench/apps/spark/hadoop-3.2.1/share/hadoop/common/lib/animal-sniffer-annotations-1.17.jar:/users/skannan/ssd/NVM/appbench/apps/spark/hadoop-3.2.1/share/hadoop/common/lib/snappy-java-1.0.5.jar:/users/skannan/ssd/NVM/appbench/apps/spark/hadoop-3.2.1/share/hadoop/common/lib/jetty-io-9.3.24.v20180605.jar:/users/skannan/ssd/NVM/appbench/apps/spark/hadoop-3.2.1/share/hadoop/common/lib/curator-framework-2.13.0.jar:/users/skannan/ssd/NVM/appbench/apps/spark/hadoop-3.2.1/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/users/skannan/ssd/NVM/appbench/apps/spark/hadoop-3.2.1/share/hadoop/common/lib/jettison-1.1.jar:/users/skannan/ssd/NVM/appbench/apps/spark/hadoop-3.2.1/share/hadoop/common/lib/jcip-annotations-1.0-1.jar:/users/skannan/ssd/NVM/appbench/apps/spark/hadoop-3.2.1/share/hadoop/common/lib/javax.servlet-api-3.1.0.jar:/users/skannan/ssd/NVM/appbench/apps/spark/hadoop-3.2.1/share/hadoop/common/lib/checker-qual-2.5.2.jar:/users/skannan/ssd/NVM/appbench/apps/spark/hadoop-3.2.1/share/hadoop/common/lib/jackson-databind-2.9.8.jar:/users/skannan/ssd/NVM/appbench/apps/spark/hadoop-3.2.1/share/hadoop/common/lib/jul-to-slf4j-1.7.25.jar:/users/skannan/ssd/NVM/appbench/apps/spark/hadoop-3.2.1/share/hadoop/common/lib/jetty-security-9.3.24.v20180605.jar:/users/skannan/ssd/NVM/appbench/apps/spark/hadoop-3.2.1/share/hadoop/common/lib/zookeeper-3.4.13.jar:/users/skannan/ssd/NVM/appbench/apps/spark/hadoop-3.2.1/share/hadoop/common/lib/commons-text-1.4.jar:/users/skannan/ssd/NVM/appbench/apps/spark/hadoop-3.2.1/share/hadoop/common/lib/woodstox-core-5.0.3.jar:/users/skannan/ssd/NVM/appbench/apps/spark/hadoop-3.2.1/share/hadoop/common/lib/commons-configuration2-2.1.1.jar:/users/skannan/ssd/NVM/appbench/apps/spark/hadoop-3.2.1/share/hadoop/common/lib/curator-recipes-2.13.0.jar:/users/skannan/ssd/NVM/appbench/apps/spark/hadoop-3.2.1/share/hadoop/common/lib/jetty-server-9.3.24.v20180605.jar:/users/skannan/ssd/NVM/appbench/apps/spark/hadoop-3.2.1/share/hadoop/common/lib/slf4j-log4j12-1.7.25.jar:/users/skannan/ssd/NVM/appbench/apps/spark/hadoop-3.2.1/share/hadoop/common/lib/kerby-util-1.0.1.jar:/users/skannan/ssd/NVM/appbench/apps/spark/hadoop-3.2.1/share/hadoop/common/lib/stax2-api-3.1.4.jar:/users/skannan/ssd/NVM/appbench/apps/spark/hadoop-3.2.1/share/hadoop/common/lib/jsr311-api-1.1.1.jar:/users/skannan/ssd/NVM/appbench/apps/spark/hadoop-3.2.1/share/hadoop/common/lib/asm-5.0.4.jar:/users/skannan/ssd/NVM/appbench/apps/spark/hadoop-3.2.1/share/hadoop/common/lib/commons-codec-1.11.jar:/users/skannan/ssd/NVM/appbench/apps/spark/hadoop-3.2.1/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/users/skannan/ssd/NVM/appbench/apps/spark/hadoop-3.2.1/share/hadoop/common/lib/json-smart-2.3.jar:/users/skannan/ssd/NVM/appbench/apps/spark/hadoop-3.2.1/share/hadoop/common/lib/log4j-1.2.17.jar:/users/skannan/ssd/NVM/appbench/apps/spark/hadoop-3.2.1/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/users/skannan/ssd/NVM/appbench/apps/spark/hadoop-3.2.1/share/hadoop/common/lib/audience-annotations-0.5.0.jar:/users/skannan/ssd/NVM/appbench/apps/spark/hadoop-3.2.1/share/hadoop/common/lib/commons-collections-3.2.2.jar:/users/skannan/ssd/NVM/appbench/apps/spark/hadoop-3.2.1/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/users/skannan/ssd/NVM/appbench/apps/spark/hadoop-3.2.1/share/hadoop/common/lib/kerb-util-1.0.1.jar:/users/skannan/ssd/NVM/appbench/apps/spark/hadoop-3.2.1/share/hadoop/common/lib/accessors-smart-1.2.jar:/users/skannan/ssd/NVM/appbench/apps/spark/hadoop-3.2.1/share/hadoop/common/lib/kerb-client-1.0.1.jar:/users/skannan/ssd/NVM/appbench/apps/spark/hadoop-3.2.1/share/hadoop/common/lib/jersey-servlet-1.19.jar:/users/skannan/ssd/NVM/appbench/apps/spark/hadoop-3.2.1/share/hadoop/common/lib/kerb-core-1.0.1.jar:/users/skannan/ssd/NVM/appbench/apps/spark/hadoop-3.2.1/share/hadoop/common/lib/commons-cli-1.2.jar:/users/skannan/ssd/NVM/appbench/apps/spark/hadoop-3.2.1/share/hadoop/common/lib/slf4j-api-1.7.25.jar:/users/skannan/ssd/NVM/appbench/apps/spark/hadoop-3.2.1/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/users/skannan/ssd/NVM/appbench/apps/spark/hadoop-3.2.1/share/hadoop/common/lib/jersey-server-1.19.jar:/users/skannan/ssd/NVM/appbench/apps/spark/hadoop-3.2.1/share/hadoop/common/lib/jsr305-3.0.0.jar:/users/skannan/ssd/NVM/appbench/apps/spark/hadoop-3.2.1/share/hadoop/common/lib/kerb-common-1.0.1.jar:/users/skannan/ssd/NVM/appbench/apps/spark/hadoop-3.2.1/share/hadoop/common/lib/commons-io-2.5.jar:/users/skannan/ssd/NVM/appbench/apps/spark/hadoop-3.2.1/share/hadoop/common/lib/commons-math3-3.1.1.jar:/users/skannan/ssd/NVM/appbench/apps/spark/hadoop-3.2.1/share/hadoop/common/lib/token-provider-1.0.1.jar:/users/skannan/ssd/NVM/appbench/apps/spark/hadoop-3.2.1/share/hadoop/common/lib/avro-1.7.7.jar:/users/skannan/ssd/NVM/appbench/apps/spark/hadoop-3.2.1/share/hadoop/common/lib/httpcore-4.4.10.jar:/users/skannan/ssd/NVM/appbench/apps/spark/hadoop-3.2.1/share/hadoop/common/lib/jackson-annotations-2.9.8.jar:/users/skannan/ssd/NVM/appbench/apps/spark/hadoop-3.2.1/share/hadoop/common/lib/jackson-core-2.9.8.jar:/users/skannan/ssd/NVM/appbench/apps/spark/hadoop-3.2.1/share/hadoop/common/lib/kerb-crypto-1.0.1.jar:/users/skannan/ssd/NVM/appbench/apps/spark/hadoop-3.2.1/share/hadoop/common/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/users/skannan/ssd/NVM/appbench/apps/spark/hadoop-3.2.1/share/hadoop/common/lib/kerb-admin-1.0.1.jar:/users/skannan/ssd/NVM/appbench/apps/spark/hadoop-3.2.1/share/hadoop/common/lib/kerby-config-1.0.1.jar:/users/skannan/ssd/NVM/appbench/apps/spark/hadoop-3.2.1/share/hadoop/common/lib/jetty-webapp-9.3.24.v20180605.jar:/users/skannan/ssd/NVM/appbench/apps/spark/hadoop-3.2.1/share/hadoop/common/lib/hadoop-annotations-3.2.1.jar:/users/skannan/ssd/NVM/appbench/apps/spark/hadoop-3.2.1/share/hadoop/common/lib/jsp-api-2.1.jar:/users/skannan/ssd/NVM/appbench/apps/spark/hadoop-3.2.1/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/users/skannan/ssd/NVM/appbench/apps/spark/hadoop-3.2.1/share/hadoop/common/lib/kerby-asn1-1.0.1.jar:/users/skannan/ssd/NVM/appbench/apps/spark/hadoop-3.2.1/share/hadoop/common/lib/nimbus-jose-jwt-4.41.1.jar:/users/skannan/ssd/NVM/appbench/apps/spark/hadoop-3.2.1/share/hadoop/common/lib/j2objc-annotations-1.1.jar:/users/skannan/ssd/NVM/appbench/apps/spark/hadoop-3.2.1/share/hadoop/common/lib/commons-lang3-3.7.jar:/users/skannan/ssd/NVM/appbench/apps/spark/hadoop-3.2.1/share/hadoop/common/lib/jetty-util-9.3.24.v20180605.jar:/users/skannan/ssd/NVM/appbench/apps/spark/hadoop-3.2.1/share/hadoop/common/lib/error_prone_annotations-2.2.0.jar:/users/skannan/ssd/NVM/appbench/apps/spark/hadoop-3.2.1/share/hadoop/common/lib/kerby-pkix-1.0.1.jar:/users/skannan/ssd/NVM/appbench/apps/spark/hadoop-3.2.1/share/hadoop/common/lib/kerby-xdr-1.0.1.jar:/users/skannan/ssd/NVM/appbench/apps/spark/hadoop-3.2.1/share/hadoop/common/lib/commons-net-3.6.jar:/users/skannan/ssd/NVM/appbench/apps/spark/hadoop-3.2.1/share/hadoop/common/lib/dnsjava-2.1.7.jar:/users/skannan/ssd/NVM/appbench/apps/spark/hadoop-3.2.1/share/hadoop/common/lib/gson-2.2.4.jar:/users/skannan/ssd/NVM/appbench/apps/spark/hadoop-3.2.1/share/hadoop/common/lib/metrics-core-3.2.4.jar:/users/skannan/ssd/NVM/appbench/apps/spark/hadoop-3.2.1/share/hadoop/common/lib/kerb-server-1.0.1.jar:/users/skannan/ssd/NVM/appbench/apps/spark/hadoop-3.2.1/share/hadoop/common/lib/jetty-xml-9.3.24.v20180605.jar:/users/skannan/ssd/NVM/appbench/apps/spark/hadoop-3.2.1/share/hadoop/common/lib/commons-beanutils-1.9.3.jar:/users/skannan/ssd/NVM/appbench/apps/spark/hadoop-3.2.1/share/hadoop/common/lib/jetty-http-9.3.24.v20180605.jar:/users/skannan/ssd/NVM/appbench/apps/spark/hadoop-3.2.1/share/hadoop/common/lib/jetty-servlet-9.3.24.v20180605.jar:/users/skannan/ssd/NVM/appbench/apps/spark/hadoop-3.2.1/share/hadoop/common/lib/jaxb-api-2.2.11.jar:/users/skannan/ssd/NVM/appbench/apps/spark/hadoop-3.2.1/share/hadoop/common/lib/curator-client-2.13.0.jar:/users/skannan/ssd/NVM/appbench/apps/spark/hadoop-3.2.1/share/hadoop/common/lib/httpclient-4.5.6.jar:/users/skannan/ssd/NVM/appbench/apps/spark/hadoop-3.2.1/share/hadoop/common/lib/kerb-identity-1.0.1.jar:/users/skannan/ssd/NVM/appbench/apps/spark/hadoop-3.2.1/share/hadoop/common/lib/commons-logging-1.1.3.jar:/users/skannan/ssd/NVM/appbench/apps/spark/hadoop-3.2.1/share/hadoop/common/lib/commons-compress-1.18.jar:/users/skannan/ssd/NVM/appbench/apps/spark/hadoop-3.2.1/share/hadoop/common/lib/guava-27.0-jre.jar:/users/skannan/ssd/NVM/appbench/apps/spark/hadoop-3.2.1/share/hadoop/common/lib/re2j-1.1.jar:/users/skannan/ssd/NVM/appbench/apps/spark/hadoop-3.2.1/share/hadoop/common/lib/failureaccess-1.0.jar:/users/skannan/ssd/NVM/appbench/apps/spark/hadoop-3.2.1/share/hadoop/common/lib/netty-3.10.5.Final.jar:/users/skannan/ssd/NVM/appbench/apps/spark/hadoop-3.2.1/share/hadoop/common/lib/htrace-core4-4.1.0-incubating.jar:/users/skannan/ssd/NVM/appbench/apps/spark/hadoop-3.2.1/share/hadoop/common/lib/kerb-simplekdc-1.0.1.jar:/users/skannan/ssd/NVM/appbench/apps/spark/hadoop-3.2.1/share/hadoop/common/hadoop-kms-3.2.1.jar:/users/skannan/ssd/NVM/appbench/apps/spark/hadoop-3.2.1/share/hadoop/common/hadoop-nfs-3.2.1.jar:/users/skannan/ssd/NVM/appbench/apps/spark/hadoop-3.2.1/share/hadoop/common/hadoop-common-3.2.1.jar:/users/skannan/ssd/NVM/appbench/apps/spark/hadoop-3.2.1/share/hadoop/common/hadoop-common-3.2.1-tests.jar:/users/skannan/ssd/NVM/appbench/apps/spark/hadoop-3.2.1/share/hadoop/hdfs:/users/skannan/ssd/NVM/appbench/apps/spark/hadoop-3.2.1/share/hadoop/hdfs/lib/paranamer-2.3.jar:/users/skannan/ssd/NVM/appbench/apps/spark/hadoop-3.2.1/share/hadoop/hdfs/lib/jsch-0.1.54.jar:/users/skannan/ssd/NVM/appbench/apps/spark/hadoop-3.2.1/share/hadoop/hdfs/lib/hadoop-auth-3.2.1.jar:/users/skannan/ssd/NVM/appbench/apps/spark/hadoop-3.2.1/share/hadoop/hdfs/lib/jersey-json-1.19.jar:/users/skannan/ssd/NVM/appbench/apps/spark/hadoop-3.2.1/share/hadoop/hdfs/lib/jersey-core-1.19.jar:/users/skannan/ssd/NVM/appbench/apps/spark/hadoop-3.2.1/share/hadoop/hdfs/lib/animal-sniffer-annotations-1.17.jar:/users/skannan/ssd/NVM/appbench/apps/spark/hadoop-3.2.1/share/hadoop/hdfs/lib/snappy-java-1.0.5.jar:/users/skannan/ssd/NVM/appbench/apps/spark/hadoop-3.2.1/share/hadoop/hdfs/lib/jetty-io-9.3.24.v20180605.jar:/users/skannan/ssd/NVM/appbench/apps/spark/hadoop-3.2.1/share/hadoop/hdfs/lib/curator-framework-2.13.0.jar:/users/skannan/ssd/NVM/appbench/apps/spark/hadoop-3.2.1/share/hadoop/hdfs/lib/jaxb-impl-2.2.3-1.jar:/users/skannan/ssd/NVM/appbench/apps/spark/hadoop-3.2.1/share/hadoop/hdfs/lib/jettison-1.1.jar:/users/skannan/ssd/NVM/appbench/apps/spark/hadoop-3.2.1/share/hadoop/hdfs/lib/jcip-annotations-1.0-1.jar:/users/skannan/ssd/NVM/appbench/apps/spark/hadoop-3.2.1/share/hadoop/hdfs/lib/javax.servlet-api-3.1.0.jar:/users/skannan/ssd/NVM/appbench/apps/spark/hadoop-3.2.1/share/hadoop/hdfs/lib/checker-qual-2.5.2.jar:/users/skannan/ssd/NVM/appbench/apps/spark/hadoop-3.2.1/share/hadoop/hdfs/lib/jackson-databind-2.9.8.jar:/users/skannan/ssd/NVM/appbench/apps/spark/hadoop-3.2.1/share/hadoop/hdfs/lib/jetty-security-9.3.24.v20180605.jar:/users/skannan/ssd/NVM/appbench/apps/spark/hadoop-3.2.1/share/hadoop/hdfs/lib/zookeeper-3.4.13.jar:/users/skannan/ssd/NVM/appbench/apps/spark/hadoop-3.2.1/share/hadoop/hdfs/lib/commons-text-1.4.jar:/users/skannan/ssd/NVM/appbench/apps/spark/hadoop-3.2.1/share/hadoop/hdfs/lib/woodstox-core-5.0.3.jar:/users/skannan/ssd/NVM/appbench/apps/spark/hadoop-3.2.1/share/hadoop/hdfs/lib/commons-configuration2-2.1.1.jar:/users/skannan/ssd/NVM/appbench/apps/spark/hadoop-3.2.1/share/hadoop/hdfs/lib/curator-recipes-2.13.0.jar:/users/skannan/ssd/NVM/appbench/apps/spark/hadoop-3.2.1/share/hadoop/hdfs/lib/jetty-server-9.3.24.v20180605.jar:/users/skannan/ssd/NVM/appbench/apps/spark/hadoop-3.2.1/share/hadoop/hdfs/lib/kerby-util-1.0.1.jar:/users/skannan/ssd/NVM/appbench/apps/spark/hadoop-3.2.1/share/hadoop/hdfs/lib/stax2-api-3.1.4.jar:/users/skannan/ssd/NVM/appbench/apps/spark/hadoop-3.2.1/share/hadoop/hdfs/lib/jsr311-api-1.1.1.jar:/users/skannan/ssd/NVM/appbench/apps/spark/hadoop-3.2.1/share/hadoop/hdfs/lib/asm-5.0.4.jar:/users/skannan/ssd/NVM/appbench/apps/spark/hadoop-3.2.1/share/hadoop/hdfs/lib/commons-codec-1.11.jar:/users/skannan/ssd/NVM/appbench/apps/spark/hadoop-3.2.1/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/users/skannan/ssd/NVM/appbench/apps/spark/hadoop-3.2.1/share/hadoop/hdfs/lib/json-smart-2.3.jar:/users/skannan/ssd/NVM/appbench/apps/spark/hadoop-3.2.1/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/users/skannan/ssd/NVM/appbench/apps/spark/hadoop-3.2.1/share/hadoop/hdfs/lib/jackson-jaxrs-1.9.13.jar:/users/skannan/ssd/NVM/appbench/apps/spark/hadoop-3.2.1/share/hadoop/hdfs/lib/audience-annotations-0.5.0.jar:/users/skannan/ssd/NVM/appbench/apps/spark/hadoop-3.2.1/share/hadoop/hdfs/lib/commons-collections-3.2.2.jar:/users/skannan/ssd/NVM/appbench/apps/spark/hadoop-3.2.1/share/hadoop/hdfs/lib/json-simple-1.1.1.jar:/users/skannan/ssd/NVM/appbench/apps/spark/hadoop-3.2.1/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/users/skannan/ssd/NVM/appbench/apps/spark/hadoop-3.2.1/share/hadoop/hdfs/lib/okio-1.6.0.jar:/users/skannan/ssd/NVM/appbench/apps/spark/hadoop-3.2.1/share/hadoop/hdfs/lib/kerb-util-1.0.1.jar:/users/skannan/ssd/NVM/appbench/apps/spark/hadoop-3.2.1/share/hadoop/hdfs/lib/accessors-smart-1.2.jar:/users/skannan/ssd/NVM/appbench/apps/spark/hadoop-3.2.1/share/hadoop/hdfs/lib/kerb-client-1.0.1.jar:/users/skannan/ssd/NVM/appbench/apps/spark/hadoop-3.2.1/share/hadoop/hdfs/lib/jersey-servlet-1.19.jar:/users/skannan/ssd/NVM/appbench/apps/spark/hadoop-3.2.1/share/hadoop/hdfs/lib/kerb-core-1.0.1.jar:/users/skannan/ssd/NVM/appbench/apps/spark/hadoop-3.2.1/share/hadoop/hdfs/lib/okhttp-2.7.5.jar:/users/skannan/ssd/NVM/appbench/apps/spark/hadoop-3.2.1/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/users/skannan/ssd/NVM/appbench/apps/spark/hadoop-3.2.1/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/users/skannan/ssd/NVM/appbench/apps/spark/hadoop-3.2.1/share/hadoop/hdfs/lib/jersey-server-1.19.jar:/users/skannan/ssd/NVM/appbench/apps/spark/hadoop-3.2.1/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/users/skannan/ssd/NVM/appbench/apps/spark/hadoop-3.2.1/share/hadoop/hdfs/lib/kerb-common-1.0.1.jar:/users/skannan/ssd/NVM/appbench/apps/spark/hadoop-3.2.1/share/hadoop/hdfs/lib/commons-io-2.5.jar:/users/skannan/ssd/NVM/appbench/apps/spark/hadoop-3.2.1/share/hadoop/hdfs/lib/commons-math3-3.1.1.jar:/users/skannan/ssd/NVM/appbench/apps/spark/hadoop-3.2.1/share/hadoop/hdfs/lib/token-provider-1.0.1.jar:/users/skannan/ssd/NVM/appbench/apps/spark/hadoop-3.2.1/share/hadoop/hdfs/lib/avro-1.7.7.jar:/users/skannan/ssd/NVM/appbench/apps/spark/hadoop-3.2.1/share/hadoop/hdfs/lib/httpcore-4.4.10.jar:/users/skannan/ssd/NVM/appbench/apps/spark/hadoop-3.2.1/share/hadoop/hdfs/lib/jackson-annotations-2.9.8.jar:/users/skannan/ssd/NVM/appbench/apps/spark/hadoop-3.2.1/share/hadoop/hdfs/lib/jackson-core-2.9.8.jar:/users/skannan/ssd/NVM/appbench/apps/spark/hadoop-3.2.1/share/hadoop/hdfs/lib/kerb-crypto-1.0.1.jar:/users/skannan/ssd/NVM/appbench/apps/spark/hadoop-3.2.1/share/hadoop/hdfs/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/users/skannan/ssd/NVM/appbench/apps/spark/hadoop-3.2.1/share/hadoop/hdfs/lib/kerb-admin-1.0.1.jar:/users/skannan/ssd/NVM/appbench/apps/spark/hadoop-3.2.1/share/hadoop/hdfs/lib/kerby-config-1.0.1.jar:/users/skannan/ssd/NVM/appbench/apps/spark/hadoop-3.2.1/share/hadoop/hdfs/lib/jetty-webapp-9.3.24.v20180605.jar:/users/skannan/ssd/NVM/appbench/apps/spark/hadoop-3.2.1/share/hadoop/hdfs/lib/hadoop-annotations-3.2.1.jar:/users/skannan/ssd/NVM/appbench/apps/spark/hadoop-3.2.1/share/hadoop/hdfs/lib/jackson-xc-1.9.13.jar:/users/skannan/ssd/NVM/appbench/apps/spark/hadoop-3.2.1/share/hadoop/hdfs/lib/kerby-asn1-1.0.1.jar:/users/skannan/ssd/NVM/appbench/apps/spark/hadoop-3.2.1/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/users/skannan/ssd/NVM/appbench/apps/spark/hadoop-3.2.1/share/hadoop/hdfs/lib/nimbus-jose-jwt-4.41.1.jar:/users/skannan/ssd/NVM/appbench/apps/spark/hadoop-3.2.1/share/hadoop/hdfs/lib/j2objc-annotations-1.1.jar:/users/skannan/ssd/NVM/appbench/apps/spark/hadoop-3.2.1/share/hadoop/hdfs/lib/commons-lang3-3.7.jar:/users/skannan/ssd/NVM/appbench/apps/spark/hadoop-3.2.1/share/hadoop/hdfs/lib/jetty-util-9.3.24.v20180605.jar:/users/skannan/ssd/NVM/appbench/apps/spark/hadoop-3.2.1/share/hadoop/hdfs/lib/jetty-util-ajax-9.3.24.v20180605.jar:/users/skannan/ssd/NVM/appbench/apps/spark/hadoop-3.2.1/share/hadoop/hdfs/lib/error_prone_annotations-2.2.0.jar:/users/skannan/ssd/NVM/appbench/apps/spark/hadoop-3.2.1/share/hadoop/hdfs/lib/kerby-pkix-1.0.1.jar:/users/skannan/ssd/NVM/appbench/apps/spark/hadoop-3.2.1/share/hadoop/hdfs/lib/kerby-xdr-1.0.1.jar:/users/skannan/ssd/NVM/appbench/apps/spark/hadoop-3.2.1/share/hadoop/hdfs/lib/commons-net-3.6.jar:/users/skannan/ssd/NVM/appbench/apps/spark/hadoop-3.2.1/share/hadoop/hdfs/lib/dnsjava-2.1.7.jar:/users/skannan/ssd/NVM/appbench/apps/spark/hadoop-3.2.1/share/hadoop/hdfs/lib/gson-2.2.4.jar:/users/skannan/ssd/NVM/appbench/apps/spark/hadoop-3.2.1/share/hadoop/hdfs/lib/kerb-server-1.0.1.jar:/users/skannan/ssd/NVM/appbench/apps/spark/hadoop-3.2.1/share/hadoop/hdfs/lib/jetty-xml-9.3.24.v20180605.jar:/users/skannan/ssd/NVM/appbench/apps/spark/hadoop-3.2.1/share/hadoop/hdfs/lib/commons-beanutils-1.9.3.jar:/users/skannan/ssd/NVM/appbench/apps/spark/hadoop-3.2.1/share/hadoop/hdfs/lib/jetty-http-9.3.24.v20180605.jar:/users/skannan/ssd/NVM/appbench/apps/spark/hadoop-3.2.1/share/hadoop/hdfs/lib/jetty-servlet-9.3.24.v20180605.jar:/users/skannan/ssd/NVM/appbench/apps/spark/hadoop-3.2.1/share/hadoop/hdfs/lib/jaxb-api-2.2.11.jar:/users/skannan/ssd/NVM/appbench/apps/spark/hadoop-3.2.1/share/hadoop/hdfs/lib/curator-client-2.13.0.jar:/users/skannan/ssd/NVM/appbench/apps/spark/hadoop-3.2.1/share/hadoop/hdfs/lib/httpclient-4.5.6.jar:/users/skannan/ssd/NVM/appbench/apps/spark/hadoop-3.2.1/share/hadoop/hdfs/lib/kerb-identity-1.0.1.jar:/users/skannan/ssd/NVM/appbench/apps/spark/hadoop-3.2.1/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/users/skannan/ssd/NVM/appbench/apps/spark/hadoop-3.2.1/share/hadoop/hdfs/lib/commons-compress-1.18.jar:/users/skannan/ssd/NVM/appbench/apps/spark/hadoop-3.2.1/share/hadoop/hdfs/lib/guava-27.0-jre.jar:/users/skannan/ssd/NVM/appbench/apps/spark/hadoop-3.2.1/share/hadoop/hdfs/lib/re2j-1.1.jar:/users/skannan/ssd/NVM/appbench/apps/spark/hadoop-3.2.1/share/hadoop/hdfs/lib/failureaccess-1.0.jar:/users/skannan/ssd/NVM/appbench/apps/spark/hadoop-3.2.1/share/hadoop/hdfs/lib/netty-3.10.5.Final.jar:/users/skannan/ssd/NVM/appbench/apps/spark/hadoop-3.2.1/share/hadoop/hdfs/lib/netty-all-4.0.52.Final.jar:/users/skannan/ssd/NVM/appbench/apps/spark/hadoop-3.2.1/share/hadoop/hdfs/lib/htrace-core4-4.1.0-incubating.jar:/users/skannan/ssd/NVM/appbench/apps/spark/hadoop-3.2.1/share/hadoop/hdfs/lib/kerb-simplekdc-1.0.1.jar:/users/skannan/ssd/NVM/appbench/apps/spark/hadoop-3.2.1/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/users/skannan/ssd/NVM/appbench/apps/spark/hadoop-3.2.1/share/hadoop/hdfs/hadoop-hdfs-rbf-3.2.1-tests.jar:/users/skannan/ssd/NVM/appbench/apps/spark/hadoop-3.2.1/share/hadoop/hdfs/hadoop-hdfs-httpfs-3.2.1.jar:/users/skannan/ssd/NVM/appbench/apps/spark/hadoop-3.2.1/share/hadoop/hdfs/hadoop-hdfs-3.2.1.jar:/users/skannan/ssd/NVM/appbench/apps/spark/hadoop-3.2.1/share/hadoop/hdfs/hadoop-hdfs-native-client-3.2.1-tests.jar:/users/skannan/ssd/NVM/appbench/apps/spark/hadoop-3.2.1/share/hadoop/hdfs/hadoop-hdfs-client-3.2.1.jar:/users/skannan/ssd/NVM/appbench/apps/spark/hadoop-3.2.1/share/hadoop/hdfs/hadoop-hdfs-rbf-3.2.1.jar:/users/skannan/ssd/NVM/appbench/apps/spark/hadoop-3.2.1/share/hadoop/hdfs/hadoop-hdfs-nfs-3.2.1.jar:/users/skannan/ssd/NVM/appbench/apps/spark/hadoop-3.2.1/share/hadoop/hdfs/hadoop-hdfs-client-3.2.1-tests.jar:/users/skannan/ssd/NVM/appbench/apps/spark/hadoop-3.2.1/share/hadoop/hdfs/hadoop-hdfs-native-client-3.2.1.jar:/users/skannan/ssd/NVM/appbench/apps/spark/hadoop-3.2.1/share/hadoop/hdfs/hadoop-hdfs-3.2.1-tests.jar:/users/skannan/ssd/NVM/appbench/apps/spark/hadoop-3.2.1/share/hadoop/mapreduce/lib/junit-4.11.jar:/users/skannan/ssd/NVM/appbench/apps/spark/hadoop-3.2.1/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/users/skannan/ssd/NVM/appbench/apps/spark/hadoop-3.2.1/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-3.2.1.jar:/users/skannan/ssd/NVM/appbench/apps/spark/hadoop-3.2.1/share/hadoop/mapreduce/hadoop-mapreduce-client-core-3.2.1.jar:/users/skannan/ssd/NVM/appbench/apps/spark/hadoop-3.2.1/share/hadoop/mapreduce/hadoop-mapreduce-client-common-3.2.1.jar:/users/skannan/ssd/NVM/appbench/apps/spark/hadoop-3.2.1/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.2.1.jar:/users/skannan/ssd/NVM/appbench/apps/spark/hadoop-3.2.1/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-3.2.1.jar:/users/skannan/ssd/NVM/appbench/apps/spark/hadoop-3.2.1/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-3.2.1-tests.jar:/users/skannan/ssd/NVM/appbench/apps/spark/hadoop-3.2.1/share/hadoop/mapreduce/hadoop-mapreduce-client-nativetask-3.2.1.jar:/users/skannan/ssd/NVM/appbench/apps/spark/hadoop-3.2.1/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-3.2.1.jar:/users/skannan/ssd/NVM/appbench/apps/spark/hadoop-3.2.1/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-3.2.1.jar:/users/skannan/ssd/NVM/appbench/apps/spark/hadoop-3.2.1/share/hadoop/mapreduce/hadoop-mapreduce-client-uploader-3.2.1.jar:/users/skannan/ssd/NVM/appbench/apps/spark/hadoop-3.2.1/share/hadoop/mapreduce/hadoop-mapreduce-client-app-3.2.1.jar:/users/skannan/ssd/NVM/appbench/apps/spark/hadoop-3.2.1/share/hadoop/yarn:/users/skannan/ssd/NVM/appbench/apps/spark/hadoop-3.2.1/share/hadoop/yarn/lib/geronimo-jcache_1.0_spec-1.0-alpha-1.jar:/users/skannan/ssd/NVM/appbench/apps/spark/hadoop-3.2.1/share/hadoop/yarn/lib/jersey-client-1.19.jar:/users/skannan/ssd/NVM/appbench/apps/spark/hadoop-3.2.1/share/hadoop/yarn/lib/jackson-jaxrs-json-provider-2.9.8.jar:/users/skannan/ssd/NVM/appbench/apps/spark/hadoop-3.2.1/share/hadoop/yarn/lib/bcprov-jdk15on-1.60.jar:/users/skannan/ssd/NVM/appbench/apps/spark/hadoop-3.2.1/share/hadoop/yarn/lib/javax.inject-1.jar:/users/skannan/ssd/NVM/appbench/apps/spark/hadoop-3.2.1/share/hadoop/yarn/lib/jackson-module-jaxb-annotations-2.9.8.jar:/users/skannan/ssd/NVM/appbench/apps/spark/hadoop-3.2.1/share/hadoop/yarn/lib/java-util-1.9.0.jar:/users/skannan/ssd/NVM/appbench/apps/spark/hadoop-3.2.1/share/hadoop/yarn/lib/guice-4.0.jar:/users/skannan/ssd/NVM/appbench/apps/spark/hadoop-3.2.1/share/hadoop/yarn/lib/guice-servlet-4.0.jar:/users/skannan/ssd/NVM/appbench/apps/spark/hadoop-3.2.1/share/hadoop/yarn/lib/jackson-jaxrs-base-2.9.8.jar:/users/skannan/ssd/NVM/appbench/apps/spark/hadoop-3.2.1/share/hadoop/yarn/lib/HikariCP-java7-2.4.12.jar:/users/skannan/ssd/NVM/appbench/apps/spark/hadoop-3.2.1/share/hadoop/yarn/lib/mssql-jdbc-6.2.1.jre7.jar:/users/skannan/ssd/NVM/appbench/apps/spark/hadoop-3.2.1/share/hadoop/yarn/lib/json-io-2.5.1.jar:/users/skannan/ssd/NVM/appbench/apps/spark/hadoop-3.2.1/share/hadoop/yarn/lib/snakeyaml-1.16.jar:/users/skannan/ssd/NVM/appbench/apps/spark/hadoop-3.2.1/share/hadoop/yarn/lib/aopalliance-1.0.jar:/users/skannan/ssd/NVM/appbench/apps/spark/hadoop-3.2.1/share/hadoop/yarn/lib/objenesis-1.0.jar:/users/skannan/ssd/NVM/appbench/apps/spark/hadoop-3.2.1/share/hadoop/yarn/lib/bcpkix-jdk15on-1.60.jar:/users/skannan/ssd/NVM/appbench/apps/spark/hadoop-3.2.1/share/hadoop/yarn/lib/ehcache-3.3.1.jar:/users/skannan/ssd/NVM/appbench/apps/spark/hadoop-3.2.1/share/hadoop/yarn/lib/fst-2.50.jar:/users/skannan/ssd/NVM/appbench/apps/spark/hadoop-3.2.1/share/hadoop/yarn/lib/metrics-core-3.2.4.jar:/users/skannan/ssd/NVM/appbench/apps/spark/hadoop-3.2.1/share/hadoop/yarn/lib/jersey-guice-1.19.jar:/users/skannan/ssd/NVM/appbench/apps/spark/hadoop-3.2.1/share/hadoop/yarn/lib/swagger-annotations-1.5.4.jar:/users/skannan/ssd/NVM/appbench/apps/spark/hadoop-3.2.1/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-3.2.1.jar:/users/skannan/ssd/NVM/appbench/apps/spark/hadoop-3.2.1/share/hadoop/yarn/hadoop-yarn-server-timeline-pluginstorage-3.2.1.jar:/users/skannan/ssd/NVM/appbench/apps/spark/hadoop-3.2.1/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-3.2.1.jar:/users/skannan/ssd/NVM/appbench/apps/spark/hadoop-3.2.1/share/hadoop/yarn/hadoop-yarn-server-router-3.2.1.jar:/users/skannan/ssd/NVM/appbench/apps/spark/hadoop-3.2.1/share/hadoop/yarn/hadoop-yarn-server-tests-3.2.1.jar:/users/skannan/ssd/NVM/appbench/apps/spark/hadoop-3.2.1/share/hadoop/yarn/hadoop-yarn-registry-3.2.1.jar:/users/skannan/ssd/NVM/appbench/apps/spark/hadoop-3.2.1/share/hadoop/yarn/hadoop-yarn-client-3.2.1.jar:/users/skannan/ssd/NVM/appbench/apps/spark/hadoop-3.2.1/share/hadoop/yarn/hadoop-yarn-services-core-3.2.1.jar:/users/skannan/ssd/NVM/appbench/apps/spark/hadoop-3.2.1/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-3.2.1.jar:/users/skannan/ssd/NVM/appbench/apps/spark/hadoop-3.2.1/share/hadoop/yarn/hadoop-yarn-submarine-3.2.1.jar:/users/skannan/ssd/NVM/appbench/apps/spark/hadoop-3.2.1/share/hadoop/yarn/hadoop-yarn-services-api-3.2.1.jar:/users/skannan/ssd/NVM/appbench/apps/spark/hadoop-3.2.1/share/hadoop/yarn/hadoop-yarn-server-nodemanager-3.2.1.jar:/users/skannan/ssd/NVM/appbench/apps/spark/hadoop-3.2.1/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-3.2.1.jar:/users/skannan/ssd/NVM/appbench/apps/spark/hadoop-3.2.1/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-3.2.1.jar:/users/skannan/ssd/NVM/appbench/apps/spark/hadoop-3.2.1/share/hadoop/yarn/hadoop-yarn-server-common-3.2.1.jar:/users/skannan/ssd/NVM/appbench/apps/spark/hadoop-3.2.1/share/hadoop/yarn/hadoop-yarn-common-3.2.1.jar:/users/skannan/ssd/NVM/appbench/apps/spark/hadoop-3.2.1/share/hadoop/yarn/hadoop-yarn-api-3.2.1.jar:/users/skannan/ssd/NVM/appbench/apps/spark/hadoop-3.2.1/share/hadoop/yarn/hadoop-yarn-server-web-proxy-3.2.1.jar
STARTUP_MSG:   build = https://gitbox.apache.org/repos/asf/hadoop.git -r b3cbbb467e22ea829b3808f4b7b01d07e0bf3842; compiled by 'rohithsharmaks' on 2019-09-10T15:56Z
STARTUP_MSG:   java = 1.8.0_222
************************************************************/
2019-10-30 08:06:34,839 INFO namenode.NameNode: registered UNIX signal handlers for [TERM, HUP, INT]
2019-10-30 08:06:35,345 INFO namenode.NameNode: createNameNode [-format]
Formatting using clusterid: CID-a9849812-8fea-46c9-971f-b9eb17cf410a
2019-10-30 08:06:38,341 INFO namenode.FSEditLog: Edit logging is async:true
2019-10-30 08:06:38,474 INFO namenode.FSNamesystem: KeyProvider: null
2019-10-30 08:06:38,488 INFO namenode.FSNamesystem: fsLock is fair: true
2019-10-30 08:06:38,493 INFO namenode.FSNamesystem: Detailed lock hold time metrics enabled: false
2019-10-30 08:06:38,520 INFO namenode.FSNamesystem: fsOwner             = skannan (auth:SIMPLE)
2019-10-30 08:06:38,521 INFO namenode.FSNamesystem: supergroup          = supergroup
2019-10-30 08:06:38,521 INFO namenode.FSNamesystem: isPermissionEnabled = true
2019-10-30 08:06:38,522 INFO namenode.FSNamesystem: HA Enabled: false
2019-10-30 08:06:38,923 INFO common.Util: dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling
2019-10-30 08:06:38,999 INFO blockmanagement.DatanodeManager: dfs.block.invalidate.limit: configured=1000, counted=60, effected=1000
2019-10-30 08:06:39,000 INFO blockmanagement.DatanodeManager: dfs.namenode.datanode.registration.ip-hostname-check=true
2019-10-30 08:06:39,026 INFO blockmanagement.BlockManager: dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000
2019-10-30 08:06:39,030 INFO blockmanagement.BlockManager: The block deletion will start around 2019 Oct 30 08:06:39
2019-10-30 08:06:39,051 INFO util.GSet: Computing capacity for map BlocksMap
2019-10-30 08:06:39,052 INFO util.GSet: VM type       = 64-bit
2019-10-30 08:06:39,060 INFO util.GSet: 2.0% max memory 26.7 GB = 546.1 MB
2019-10-30 08:06:39,061 INFO util.GSet: capacity      = 2^26 = 67108864 entries
2019-10-30 08:06:39,687 INFO blockmanagement.BlockManager: Storage policy satisfier is disabled
2019-10-30 08:06:39,689 INFO blockmanagement.BlockManager: dfs.block.access.token.enable = false
2019-10-30 08:06:39,721 INFO Configuration.deprecation: No unit for dfs.namenode.safemode.extension(30000) assuming MILLISECONDS
2019-10-30 08:06:39,723 INFO blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.threshold-pct = 0.9990000128746033
2019-10-30 08:06:39,723 INFO blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.min.datanodes = 0
2019-10-30 08:06:39,723 INFO blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.extension = 30000
2019-10-30 08:06:39,728 INFO blockmanagement.BlockManager: defaultReplication         = 1
2019-10-30 08:06:39,728 INFO blockmanagement.BlockManager: maxReplication             = 512
2019-10-30 08:06:39,728 INFO blockmanagement.BlockManager: minReplication             = 1
2019-10-30 08:06:39,728 INFO blockmanagement.BlockManager: maxReplicationStreams      = 2
2019-10-30 08:06:39,728 INFO blockmanagement.BlockManager: redundancyRecheckInterval  = 3000ms
2019-10-30 08:06:39,728 INFO blockmanagement.BlockManager: encryptDataTransfer        = false
2019-10-30 08:06:39,728 INFO blockmanagement.BlockManager: maxNumBlocksToLog          = 1000
2019-10-30 08:06:39,904 INFO namenode.FSDirectory: GLOBAL serial map: bits=29 maxEntries=536870911
2019-10-30 08:06:39,904 INFO namenode.FSDirectory: USER serial map: bits=24 maxEntries=16777215
2019-10-30 08:06:39,904 INFO namenode.FSDirectory: GROUP serial map: bits=24 maxEntries=16777215
2019-10-30 08:06:39,905 INFO namenode.FSDirectory: XATTR serial map: bits=24 maxEntries=16777215
2019-10-30 08:06:39,975 INFO util.GSet: Computing capacity for map INodeMap
2019-10-30 08:06:39,975 INFO util.GSet: VM type       = 64-bit
2019-10-30 08:06:39,977 INFO util.GSet: 1.0% max memory 26.7 GB = 273.0 MB
2019-10-30 08:06:39,977 INFO util.GSet: capacity      = 2^25 = 33554432 entries
2019-10-30 08:06:42,384 INFO namenode.FSDirectory: ACLs enabled? false
2019-10-30 08:06:42,386 INFO namenode.FSDirectory: POSIX ACL inheritance enabled? true
2019-10-30 08:06:42,387 INFO namenode.FSDirectory: XAttrs enabled? true
2019-10-30 08:06:42,388 INFO namenode.NameNode: Caching file names occurring more than 10 times
2019-10-30 08:06:42,433 INFO snapshot.SnapshotManager: Loaded config captureOpenFiles: false, skipCaptureAccessTimeOnlyChange: false, snapshotDiffAllowSnapRootDescendant: true, maxSnapshotLimit: 65536
2019-10-30 08:06:42,452 INFO snapshot.SnapshotManager: SkipList is disabled
2019-10-30 08:06:42,491 INFO util.GSet: Computing capacity for map cachedBlocks
2019-10-30 08:06:42,492 INFO util.GSet: VM type       = 64-bit
2019-10-30 08:06:42,495 INFO util.GSet: 0.25% max memory 26.7 GB = 68.3 MB
2019-10-30 08:06:42,495 INFO util.GSet: capacity      = 2^23 = 8388608 entries
2019-10-30 08:06:42,591 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.window.num.buckets = 10
2019-10-30 08:06:42,592 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.num.users = 10
2019-10-30 08:06:42,592 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.windows.minutes = 1,5,25
2019-10-30 08:06:42,608 INFO namenode.FSNamesystem: Retry cache on namenode is enabled
2019-10-30 08:06:42,609 INFO namenode.FSNamesystem: Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis
2019-10-30 08:06:42,618 INFO util.GSet: Computing capacity for map NameNodeRetryCache
2019-10-30 08:06:42,619 INFO util.GSet: VM type       = 64-bit
2019-10-30 08:06:42,621 INFO util.GSet: 0.029999999329447746% max memory 26.7 GB = 8.2 MB
2019-10-30 08:06:42,621 INFO util.GSet: capacity      = 2^20 = 1048576 entries
2019-10-30 08:06:42,818 INFO namenode.FSImage: Allocated new BlockPoolId: BP-744754940-127.0.0.1-1572444402768
2019-10-30 08:06:42,943 INFO common.Storage: Storage directory /users/skannan/ssd/NVM/appbench/apps/spark/hadoop-3.2.1/hdfs/namenode has been successfully formatted.
2019-10-30 08:06:43,171 INFO namenode.FSImageFormatProtobuf: Saving image file /users/skannan/ssd/NVM/appbench/apps/spark/hadoop-3.2.1/hdfs/namenode/current/fsimage.ckpt_0000000000000000000 using no compression
2019-10-30 08:06:44,207 INFO namenode.FSImageFormatProtobuf: Image file /users/skannan/ssd/NVM/appbench/apps/spark/hadoop-3.2.1/hdfs/namenode/current/fsimage.ckpt_0000000000000000000 of size 402 bytes saved in 1 seconds .
2019-10-30 08:06:44,302 INFO namenode.NNStorageRetentionManager: Going to retain 1 images with txid >= 0
2019-10-30 08:06:44,343 INFO namenode.FSImage: FSImageSaver clean checkpoint: txid=0 when meet shutdown.
2019-10-30 08:06:44,352 INFO namenode.NameNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down NameNode at node-1.16-04-kernel.fsperfatscale-pg0.wisc.cloudlab.us/127.0.0.1
************************************************************/
+ sleep 2
+ start-dfs.sh
Starting namenodes on [localhost]
Starting datanodes
Starting secondary namenodes [localhost]
application termination...
+ start-yarn.sh
Starting resourcemanager
Starting nodemanagers
+ Terasort/bin/gen_data.sh
========== preparing Terasort data ==========
application termination...
Master:
sh -c  /users/skannan/ssd/NVM/appbench/apps/spark/bin/spark-submit --class src.main.scala.terasortDataGen --master local[*]   --conf spark.rpc.askTimeout=500 --conf spark.executor.memory=7g --conf spark.serializer=org.apache.spark.serializer.KryoSerializer --conf spark.rdd.compress=false --conf spark.io.compression.codec=lzf --jars /users/skannan/ssd/NVM/appbench/apps/spark/spark-bench/Terasort/target/jars/guava-19.0-rc2.jar /users/skannan/ssd/NVM/appbench/apps/spark/spark-bench/Terasort/target/TerasortApp-1.0-jar-with-dependencies.jar  10000000 hdfs://localhost:8020/SparkBench/Terasort/Input 10 2>&1|tee /users/skannan/ssd/NVM/appbench/apps/spark/spark-bench/bin/..//num/Terasort_gendata_2019-10-30-08:07:20.dat
+ '[' -z /users/skannan/ssd/NVM/appbench/apps/spark ']'
+ export PYTHONHASHSEED=0
+ PYTHONHASHSEED=0
+ exec /users/skannan/ssd/NVM/appbench/apps/spark/bin/spark-class org.apache.spark.deploy.SparkSubmit --class src.main.scala.terasortDataGen --master 'local[*]' --conf spark.rpc.askTimeout=500 --conf spark.executor.memory=7g --conf spark.serializer=org.apache.spark.serializer.KryoSerializer --conf spark.rdd.compress=false --conf spark.io.compression.codec=lzf --jars /users/skannan/ssd/NVM/appbench/apps/spark/spark-bench/Terasort/target/jars/guava-19.0-rc2.jar /users/skannan/ssd/NVM/appbench/apps/spark/spark-bench/Terasort/target/TerasortApp-1.0-jar-with-dependencies.jar 10000000 hdfs://localhost:8020/SparkBench/Terasort/Input 10
Unescaped left brace in regex is deprecated, passed through in regex; marked by <-- HERE in m/%{ <-- HERE (.*?)}/ at /usr/bin/print line 528.
Error: no "print" mailcap rules found for type "application/x-executable"
Unescaped left brace in regex is deprecated, passed through in regex; marked by <-- HERE in m/%{ <-- HERE (.*?)}/ at /usr/bin/print line 528.
Error: no "print" mailcap rules found for type "application/x-executable"
Unescaped left brace in regex is deprecated, passed through in regex; marked by <-- HERE in m/%{ <-- HERE (.*?)}/ at /usr/bin/print line 528.
Error: no "print" mailcap rules found for type "application/x-executable"
Unescaped left brace in regex is deprecated, passed through in regex; marked by <-- HERE in m/%{ <-- HERE (.*?)}/ at /usr/bin/print line 528.
Error: no "print" mailcap rules found for type "application/x-executable"
Unescaped left brace in regex is deprecated, passed through in regex; marked by <-- HERE in m/%{ <-- HERE (.*?)}/ at /usr/bin/print line 528.
Error: no "print" mailcap rules found for type "application/x-executable"
Unescaped left brace in regex is deprecated, passed through in regex; marked by <-- HERE in m/%{ <-- HERE (.*?)}/ at /usr/bin/print line 528.
Error: no "print" mailcap rules found for type "application/x-executable"
Unescaped left brace in regex is deprecated, passed through in regex; marked by <-- HERE in m/%{ <-- HERE (.*?)}/ at /usr/bin/print line 528.
Error: no "print" mailcap rules found for type "application/x-executable"
Unescaped left brace in regex is deprecated, passed through in regex; marked by <-- HERE in m/%{ <-- HERE (.*?)}/ at /usr/bin/print line 528.
Error: no "print" mailcap rules found for type "application/x-executable"
Unescaped left brace in regex is deprecated, passed through in regex; marked by <-- HERE in m/%{ <-- HERE (.*?)}/ at /usr/bin/print line 528.
Error: no "print" mailcap rules found for type "application/x-executable"
Unescaped left brace in regex is deprecated, passed through in regex; marked by <-- HERE in m/%{ <-- HERE (.*?)}/ at /usr/bin/print line 528.
Error: no "print" mailcap rules found for type "application/x-executable"
Unescaped left brace in regex is deprecated, passed through in regex; marked by <-- HERE in m/%{ <-- HERE (.*?)}/ at /usr/bin/print line 528.
Error: no "print" mailcap rules found for type "application/x-executable"
Unescaped left brace in regex is deprecated, passed through in regex; marked by <-- HERE in m/%{ <-- HERE (.*?)}/ at /usr/bin/print line 528.
Error: no "print" mailcap rules found for type "application/x-executable"
Unescaped left brace in regex is deprecated, passed through in regex; marked by <-- HERE in m/%{ <-- HERE (.*?)}/ at /usr/bin/print line 528.
Error: no "print" mailcap rules found for type "application/x-executable"
Unescaped left brace in regex is deprecated, passed through in regex; marked by <-- HERE in m/%{ <-- HERE (.*?)}/ at /usr/bin/print line 528.
Error: no "print" mailcap rules found for type "application/x-executable"
Unescaped left brace in regex is deprecated, passed through in regex; marked by <-- HERE in m/%{ <-- HERE (.*?)}/ at /usr/bin/print line 528.
Error: no "print" mailcap rules found for type "application/x-executable"
Unescaped left brace in regex is deprecated, passed through in regex; marked by <-- HERE in m/%{ <-- HERE (.*?)}/ at /usr/bin/print line 528.
Error: no "print" mailcap rules found for type "application/x-executable"
Unescaped left brace in regex is deprecated, passed through in regex; marked by <-- HERE in m/%{ <-- HERE (.*?)}/ at /usr/bin/print line 528.
Error: no "print" mailcap rules found for type "application/x-executable"
Unescaped left brace in regex is deprecated, passed through in regex; marked by <-- HERE in m/%{ <-- HERE (.*?)}/ at /usr/bin/print line 528.
Error: no "print" mailcap rules found for type "application/x-executable"
Unescaped left brace in regex is deprecated, passed through in regex; marked by <-- HERE in m/%{ <-- HERE (.*?)}/ at /usr/bin/print line 528.
Error: no "print" mailcap rules found for type "application/x-executable"
Unescaped left brace in regex is deprecated, passed through in regex; marked by <-- HERE in m/%{ <-- HERE (.*?)}/ at /usr/bin/print line 528.
Error: no "print" mailcap rules found for type "application/x-executable"
Unescaped left brace in regex is deprecated, passed through in regex; marked by <-- HERE in m/%{ <-- HERE (.*?)}/ at /usr/bin/print line 528.
Error: no "print" mailcap rules found for type "application/x-executable"
Unescaped left brace in regex is deprecated, passed through in regex; marked by <-- HERE in m/%{ <-- HERE (.*?)}/ at /usr/bin/print line 528.
Error: no "print" mailcap rules found for type "application/x-executable"
Unescaped left brace in regex is deprecated, passed through in regex; marked by <-- HERE in m/%{ <-- HERE (.*?)}/ at /usr/bin/print line 528.
Error: no "print" mailcap rules found for type "application/x-executable"
Unescaped left brace in regex is deprecated, passed through in regex; marked by <-- HERE in m/%{ <-- HERE (.*?)}/ at /usr/bin/print line 528.
Error: no "print" mailcap rules found for type "application/x-executable"
Unescaped left brace in regex is deprecated, passed through in regex; marked by <-- HERE in m/%{ <-- HERE (.*?)}/ at /usr/bin/print line 528.
Error: no "print" mailcap rules found for type "application/x-executable"
Unescaped left brace in regex is deprecated, passed through in regex; marked by <-- HERE in m/%{ <-- HERE (.*?)}/ at /usr/bin/print line 528.
Error: no "print" mailcap rules found for type "application/x-executable"
Unescaped left brace in regex is deprecated, passed through in regex; marked by <-- HERE in m/%{ <-- HERE (.*?)}/ at /usr/bin/print line 528.
Error: no "print" mailcap rules found for type "application/x-executable"
2019-10-30 08:07:29,170 WARN util.Utils: Your hostname, node-1.16-04-kernel.fsperfatscale-pg0.wisc.cloudlab.us resolves to a loopback address: 127.0.0.1; using 128.105.145.84 instead (on interface enp1s0f0)
2019-10-30 08:07:29,174 WARN util.Utils: Set SPARK_LOCAL_IP if you need to bind to another address
2019-10-30 08:07:31,356 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2019-10-30 08:07:36,533 INFO util.log: Logging initialized @10652ms
2019-10-30 08:07:36,757 INFO server.Server: jetty-9.3.z-SNAPSHOT, build timestamp: unknown, git hash: unknown
2019-10-30 08:07:36,821 INFO server.Server: Started @10945ms
2019-10-30 08:07:36,912 INFO server.AbstractConnector: Started ServerConnector@3df8bcdf{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2019-10-30 08:07:37,072 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@4e2c95ee{/jobs,null,AVAILABLE,@Spark}
2019-10-30 08:07:37,075 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@27e32fe4{/jobs/json,null,AVAILABLE,@Spark}
2019-10-30 08:07:37,076 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@c3c4c1c{/jobs/job,null,AVAILABLE,@Spark}
2019-10-30 08:07:37,080 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@3d7cc3cb{/jobs/job/json,null,AVAILABLE,@Spark}
2019-10-30 08:07:37,089 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@35e478f{/stages,null,AVAILABLE,@Spark}
2019-10-30 08:07:37,092 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@6d6cb754{/stages/json,null,AVAILABLE,@Spark}
2019-10-30 08:07:37,095 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@6b7d1df8{/stages/stage,null,AVAILABLE,@Spark}
2019-10-30 08:07:37,104 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@49096b06{/stages/stage/json,null,AVAILABLE,@Spark}
2019-10-30 08:07:37,106 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@4a183d02{/stages/pool,null,AVAILABLE,@Spark}
2019-10-30 08:07:37,108 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@5d05ef57{/stages/pool/json,null,AVAILABLE,@Spark}
2019-10-30 08:07:37,109 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@213deac2{/storage,null,AVAILABLE,@Spark}
2019-10-30 08:07:37,111 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@23eee4b8{/storage/json,null,AVAILABLE,@Spark}
2019-10-30 08:07:37,112 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@28952dea{/storage/rdd,null,AVAILABLE,@Spark}
2019-10-30 08:07:37,114 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@5a9800f8{/storage/rdd/json,null,AVAILABLE,@Spark}
2019-10-30 08:07:37,116 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@143d9a93{/environment,null,AVAILABLE,@Spark}
2019-10-30 08:07:37,118 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@40226788{/environment/json,null,AVAILABLE,@Spark}
2019-10-30 08:07:37,119 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@4159e81b{/executors,null,AVAILABLE,@Spark}
2019-10-30 08:07:37,121 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@5b5caf08{/executors/json,null,AVAILABLE,@Spark}
2019-10-30 08:07:37,123 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@23cd4ff2{/executors/threadDump,null,AVAILABLE,@Spark}
2019-10-30 08:07:37,124 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@70807224{/executors/threadDump/json,null,AVAILABLE,@Spark}
2019-10-30 08:07:37,146 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@7e97551f{/static,null,AVAILABLE,@Spark}
2019-10-30 08:07:37,152 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@267bbe1a{/,null,AVAILABLE,@Spark}
2019-10-30 08:07:37,155 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@1f12e153{/api,null,AVAILABLE,@Spark}
2019-10-30 08:07:37,176 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@29f0802c{/jobs/job/kill,null,AVAILABLE,@Spark}
2019-10-30 08:07:37,179 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@3a60c416{/stages/stage/kill,null,AVAILABLE,@Spark}
2019-10-30 08:07:38,481 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@67b7c170{/metrics/json,null,AVAILABLE,@Spark}
===========================================================================
===========================================================================
Input size: 1000MB
Total number of records: 10000000
Number of output partitions: 10
Number of records/ partition:   1000000
===========================================================================
===========================================================================
2019-10-30 08:07:39,903 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1
2019-10-30 08:07:45,429 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1
2019-10-30 08:07:45,430 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1
2019-10-30 08:07:45,435 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1
2019-10-30 08:07:45,435 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1
2019-10-30 08:07:45,450 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1
2019-10-30 08:07:45,450 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1
2019-10-30 08:07:45,462 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1
2019-10-30 08:07:45,462 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1
2019-10-30 08:07:45,462 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1
2019-10-30 08:07:45,463 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1
2019-10-30 08:07:45,464 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1
2019-10-30 08:07:45,464 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1
2019-10-30 08:07:45,471 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1
2019-10-30 08:07:45,473 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1
2019-10-30 08:07:45,474 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1
2019-10-30 08:07:45,481 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1
2019-10-30 08:07:45,495 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1
2019-10-30 08:07:45,498 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1
2019-10-30 08:07:45,501 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1
2019-10-30 08:07:45,502 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1
2019-10-30 08:07:53,040 INFO output.FileOutputCommitter: Saved output of task 'attempt_20191030080739_0001_r_000000_0' to hdfs://localhost:8020/SparkBench/Terasort/Input/_temporary/0/task_20191030080739_0001_r_000000
2019-10-30 08:07:53,118 INFO output.FileOutputCommitter: Saved output of task 'attempt_20191030080739_0001_r_000003_0' to hdfs://localhost:8020/SparkBench/Terasort/Input/_temporary/0/task_20191030080739_0001_r_000003
2019-10-30 08:07:53,127 INFO output.FileOutputCommitter: Saved output of task 'attempt_20191030080739_0001_r_000006_0' to hdfs://localhost:8020/SparkBench/Terasort/Input/_temporary/0/task_20191030080739_0001_r_000006
2019-10-30 08:07:53,142 INFO output.FileOutputCommitter: Saved output of task 'attempt_20191030080739_0001_r_000008_0' to hdfs://localhost:8020/SparkBench/Terasort/Input/_temporary/0/task_20191030080739_0001_r_000008
2019-10-30 08:07:53,239 INFO output.FileOutputCommitter: Saved output of task 'attempt_20191030080739_0001_r_000009_0' to hdfs://localhost:8020/SparkBench/Terasort/Input/_temporary/0/task_20191030080739_0001_r_000009
2019-10-30 08:07:53,355 INFO output.FileOutputCommitter: Saved output of task 'attempt_20191030080739_0001_r_000002_0' to hdfs://localhost:8020/SparkBench/Terasort/Input/_temporary/0/task_20191030080739_0001_r_000002
2019-10-30 08:07:53,470 INFO output.FileOutputCommitter: Saved output of task 'attempt_20191030080739_0001_r_000005_0' to hdfs://localhost:8020/SparkBench/Terasort/Input/_temporary/0/task_20191030080739_0001_r_000005
2019-10-30 08:07:53,483 INFO output.FileOutputCommitter: Saved output of task 'attempt_20191030080739_0001_r_000007_0' to hdfs://localhost:8020/SparkBench/Terasort/Input/_temporary/0/task_20191030080739_0001_r_000007
2019-10-30 08:07:53,516 INFO output.FileOutputCommitter: Saved output of task 'attempt_20191030080739_0001_r_000004_0' to hdfs://localhost:8020/SparkBench/Terasort/Input/_temporary/0/task_20191030080739_0001_r_000004
2019-10-30 08:07:53,546 INFO output.FileOutputCommitter: Saved output of task 'attempt_20191030080739_0001_r_000001_0' to hdfs://localhost:8020/SparkBench/Terasort/Input/_temporary/0/task_20191030080739_0001_r_000001
Number of records written: 10000000
2019-10-30 08:07:55,681 INFO server.AbstractConnector: Stopped Spark@3df8bcdf{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
application termination...
application termination...
application termination...
+ /usr/bin/time -v Terasort/bin/run.sh
========== running Terasort benchmark ==========
application termination...
application termination...
Master:
Terasort opt hdfs://localhost:8020/SparkBench/Terasort/Input hdfs://localhost:8020/SparkBench/Terasort/Output 
application termination...
Connection to localhost closed.
data purged on localhost
sh -c  /users/skannan/ssd/NVM/appbench/apps/spark/bin/spark-submit --class src.main.scala.terasortApp --master local[*]   --conf spark.rpc.askTimeout=500 --conf spark.executor.memory=7g --conf spark.serializer=org.apache.spark.serializer.KryoSerializer --conf spark.rdd.compress=false --conf spark.io.compression.codec=lzf  --jars /users/skannan/ssd/NVM/appbench/apps/spark/spark-bench/Terasort/target/jars/guava-19.0-rc2.jar /users/skannan/ssd/NVM/appbench/apps/spark/spark-bench/Terasort/target/TerasortApp-1.0-jar-with-dependencies.jar hdfs://localhost:8020/SparkBench/Terasort/Input hdfs://localhost:8020/SparkBench/Terasort/Output  2>&1|tee /users/skannan/ssd/NVM/appbench/apps/spark/spark-bench/bin/..//num/Terasort_run_2019-10-30-08:08:21.dat
+ '[' -z /users/skannan/ssd/NVM/appbench/apps/spark ']'
+ export PYTHONHASHSEED=0
+ PYTHONHASHSEED=0
+ exec /users/skannan/ssd/NVM/appbench/apps/spark/bin/spark-class org.apache.spark.deploy.SparkSubmit --class src.main.scala.terasortApp --master 'local[*]' --conf spark.rpc.askTimeout=500 --conf spark.executor.memory=7g --conf spark.serializer=org.apache.spark.serializer.KryoSerializer --conf spark.rdd.compress=false --conf spark.io.compression.codec=lzf --jars /users/skannan/ssd/NVM/appbench/apps/spark/spark-bench/Terasort/target/jars/guava-19.0-rc2.jar /users/skannan/ssd/NVM/appbench/apps/spark/spark-bench/Terasort/target/TerasortApp-1.0-jar-with-dependencies.jar hdfs://localhost:8020/SparkBench/Terasort/Input hdfs://localhost:8020/SparkBench/Terasort/Output
Unescaped left brace in regex is deprecated, passed through in regex; marked by <-- HERE in m/%{ <-- HERE (.*?)}/ at /usr/bin/print line 528.
Error: no "print" mailcap rules found for type "application/x-executable"
Unescaped left brace in regex is deprecated, passed through in regex; marked by <-- HERE in m/%{ <-- HERE (.*?)}/ at /usr/bin/print line 528.
Error: no "print" mailcap rules found for type "application/x-executable"
Unescaped left brace in regex is deprecated, passed through in regex; marked by <-- HERE in m/%{ <-- HERE (.*?)}/ at /usr/bin/print line 528.
Error: no "print" mailcap rules found for type "application/x-executable"
Unescaped left brace in regex is deprecated, passed through in regex; marked by <-- HERE in m/%{ <-- HERE (.*?)}/ at /usr/bin/print line 528.
Error: no "print" mailcap rules found for type "application/x-executable"
Unescaped left brace in regex is deprecated, passed through in regex; marked by <-- HERE in m/%{ <-- HERE (.*?)}/ at /usr/bin/print line 528.
Error: no "print" mailcap rules found for type "application/x-executable"
Unescaped left brace in regex is deprecated, passed through in regex; marked by <-- HERE in m/%{ <-- HERE (.*?)}/ at /usr/bin/print line 528.
Error: no "print" mailcap rules found for type "application/x-executable"
Unescaped left brace in regex is deprecated, passed through in regex; marked by <-- HERE in m/%{ <-- HERE (.*?)}/ at /usr/bin/print line 528.
Error: no "print" mailcap rules found for type "application/x-executable"
Unescaped left brace in regex is deprecated, passed through in regex; marked by <-- HERE in m/%{ <-- HERE (.*?)}/ at /usr/bin/print line 528.
Error: no "print" mailcap rules found for type "application/x-executable"
Unescaped left brace in regex is deprecated, passed through in regex; marked by <-- HERE in m/%{ <-- HERE (.*?)}/ at /usr/bin/print line 528.
Error: no "print" mailcap rules found for type "application/x-executable"
Unescaped left brace in regex is deprecated, passed through in regex; marked by <-- HERE in m/%{ <-- HERE (.*?)}/ at /usr/bin/print line 528.
Error: no "print" mailcap rules found for type "application/x-executable"
Unescaped left brace in regex is deprecated, passed through in regex; marked by <-- HERE in m/%{ <-- HERE (.*?)}/ at /usr/bin/print line 528.
Error: no "print" mailcap rules found for type "application/x-executable"
Unescaped left brace in regex is deprecated, passed through in regex; marked by <-- HERE in m/%{ <-- HERE (.*?)}/ at /usr/bin/print line 528.
Error: no "print" mailcap rules found for type "application/x-executable"
Unescaped left brace in regex is deprecated, passed through in regex; marked by <-- HERE in m/%{ <-- HERE (.*?)}/ at /usr/bin/print line 528.
Error: no "print" mailcap rules found for type "application/x-executable"
Unescaped left brace in regex is deprecated, passed through in regex; marked by <-- HERE in m/%{ <-- HERE (.*?)}/ at /usr/bin/print line 528.
Error: no "print" mailcap rules found for type "application/x-executable"
Unescaped left brace in regex is deprecated, passed through in regex; marked by <-- HERE in m/%{ <-- HERE (.*?)}/ at /usr/bin/print line 528.
Error: no "print" mailcap rules found for type "application/x-executable"
Unescaped left brace in regex is deprecated, passed through in regex; marked by <-- HERE in m/%{ <-- HERE (.*?)}/ at /usr/bin/print line 528.
Error: no "print" mailcap rules found for type "application/x-executable"
Unescaped left brace in regex is deprecated, passed through in regex; marked by <-- HERE in m/%{ <-- HERE (.*?)}/ at /usr/bin/print line 528.
Error: no "print" mailcap rules found for type "application/x-executable"
Unescaped left brace in regex is deprecated, passed through in regex; marked by <-- HERE in m/%{ <-- HERE (.*?)}/ at /usr/bin/print line 528.
Error: no "print" mailcap rules found for type "application/x-executable"
Unescaped left brace in regex is deprecated, passed through in regex; marked by <-- HERE in m/%{ <-- HERE (.*?)}/ at /usr/bin/print line 528.
Error: no "print" mailcap rules found for type "application/x-executable"
Unescaped left brace in regex is deprecated, passed through in regex; marked by <-- HERE in m/%{ <-- HERE (.*?)}/ at /usr/bin/print line 528.
Error: no "print" mailcap rules found for type "application/x-executable"
Unescaped left brace in regex is deprecated, passed through in regex; marked by <-- HERE in m/%{ <-- HERE (.*?)}/ at /usr/bin/print line 528.
Error: no "print" mailcap rules found for type "application/x-executable"
Unescaped left brace in regex is deprecated, passed through in regex; marked by <-- HERE in m/%{ <-- HERE (.*?)}/ at /usr/bin/print line 528.
Error: no "print" mailcap rules found for type "application/x-executable"
Unescaped left brace in regex is deprecated, passed through in regex; marked by <-- HERE in m/%{ <-- HERE (.*?)}/ at /usr/bin/print line 528.
Error: no "print" mailcap rules found for type "application/x-executable"
Unescaped left brace in regex is deprecated, passed through in regex; marked by <-- HERE in m/%{ <-- HERE (.*?)}/ at /usr/bin/print line 528.
Error: no "print" mailcap rules found for type "application/x-executable"
Unescaped left brace in regex is deprecated, passed through in regex; marked by <-- HERE in m/%{ <-- HERE (.*?)}/ at /usr/bin/print line 528.
Error: no "print" mailcap rules found for type "application/x-executable"
Unescaped left brace in regex is deprecated, passed through in regex; marked by <-- HERE in m/%{ <-- HERE (.*?)}/ at /usr/bin/print line 528.
Error: no "print" mailcap rules found for type "application/x-executable"
2019-10-30 08:08:29,040 WARN util.Utils: Your hostname, node-1.16-04-kernel.fsperfatscale-pg0.wisc.cloudlab.us resolves to a loopback address: 127.0.0.1; using 128.105.145.84 instead (on interface enp1s0f0)
2019-10-30 08:08:29,043 WARN util.Utils: Set SPARK_LOCAL_IP if you need to bind to another address
2019-10-30 08:08:30,666 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2019-10-30 08:08:34,260 INFO util.log: Logging initialized @7556ms
2019-10-30 08:08:34,468 INFO server.Server: jetty-9.3.z-SNAPSHOT, build timestamp: unknown, git hash: unknown
2019-10-30 08:08:34,543 INFO server.Server: Started @7842ms
2019-10-30 08:08:34,636 INFO server.AbstractConnector: Started ServerConnector@51745f40{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2019-10-30 08:08:34,735 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@2fb5fe30{/jobs,null,AVAILABLE,@Spark}
2019-10-30 08:08:34,738 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@213deac2{/jobs/json,null,AVAILABLE,@Spark}
2019-10-30 08:08:34,739 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@23eee4b8{/jobs/job,null,AVAILABLE,@Spark}
2019-10-30 08:08:34,742 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@5a9800f8{/jobs/job/json,null,AVAILABLE,@Spark}
2019-10-30 08:08:34,743 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@143d9a93{/stages,null,AVAILABLE,@Spark}
2019-10-30 08:08:34,744 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@40226788{/stages/json,null,AVAILABLE,@Spark}
2019-10-30 08:08:34,745 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@4159e81b{/stages/stage,null,AVAILABLE,@Spark}
2019-10-30 08:08:34,748 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@70807224{/stages/stage/json,null,AVAILABLE,@Spark}
2019-10-30 08:08:34,750 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@7e97551f{/stages/pool,null,AVAILABLE,@Spark}
2019-10-30 08:08:34,751 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@400d912a{/stages/pool/json,null,AVAILABLE,@Spark}
2019-10-30 08:08:34,752 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@9f6e406{/storage,null,AVAILABLE,@Spark}
2019-10-30 08:08:34,754 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@7a94b64e{/storage/json,null,AVAILABLE,@Spark}
2019-10-30 08:08:34,755 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@3e6f3bae{/storage/rdd,null,AVAILABLE,@Spark}
2019-10-30 08:08:34,756 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@12477988{/storage/rdd/json,null,AVAILABLE,@Spark}
2019-10-30 08:08:34,758 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@2caf6912{/environment,null,AVAILABLE,@Spark}
2019-10-30 08:08:34,761 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@73d69c0f{/environment/json,null,AVAILABLE,@Spark}
2019-10-30 08:08:34,763 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@34237b90{/executors,null,AVAILABLE,@Spark}
2019-10-30 08:08:34,765 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@1d01dfa5{/executors/json,null,AVAILABLE,@Spark}
2019-10-30 08:08:34,766 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@4a31c2ee{/executors/threadDump,null,AVAILABLE,@Spark}
2019-10-30 08:08:34,768 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@d400943{/executors/threadDump/json,null,AVAILABLE,@Spark}
2019-10-30 08:08:34,786 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@22101c80{/static,null,AVAILABLE,@Spark}
2019-10-30 08:08:34,788 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@e8fadb0{/,null,AVAILABLE,@Spark}
2019-10-30 08:08:34,791 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@6d64b553{/api,null,AVAILABLE,@Spark}
2019-10-30 08:08:34,793 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@26a94fa5{/jobs/job/kill,null,AVAILABLE,@Spark}
2019-10-30 08:08:34,794 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@464a4442{/stages/stage/kill,null,AVAILABLE,@Spark}
2019-10-30 08:08:35,788 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@d56aaa6{/metrics/json,null,AVAILABLE,@Spark}
2019-10-30 08:08:38,901 INFO input.FileInputFormat: Total input paths to process : 10
2019-10-30 08:09:01,667 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1
2019-10-30 08:09:07,571 ERROR executor.Executor: Exception in task 1.0 in stage 3.0 (TID 21)
java.io.IOException: No space left on device
	at sun.nio.ch.FileDispatcherImpl.write0(Native Method)
	at sun.nio.ch.FileDispatcherImpl.write(FileDispatcherImpl.java:60)
	at sun.nio.ch.IOUtil.writeFromNativeBuffer(IOUtil.java:93)
	at sun.nio.ch.IOUtil.write(IOUtil.java:51)
	at sun.nio.ch.FileChannelImpl.write(FileChannelImpl.java:211)
	at sun.nio.ch.FileChannelImpl.transferToTrustedChannel(FileChannelImpl.java:516)
	at sun.nio.ch.FileChannelImpl.transferTo(FileChannelImpl.java:609)
	at org.apache.spark.util.Utils$.copyFileStreamNIO(Utils.scala:389)
	at org.apache.spark.util.Utils$$anonfun$copyStream$1.apply$mcJ$sp(Utils.scala:354)
	at org.apache.spark.util.Utils$$anonfun$copyStream$1.apply(Utils.scala:348)
	at org.apache.spark.util.Utils$$anonfun$copyStream$1.apply(Utils.scala:348)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.util.Utils$.copyStream(Utils.scala:369)
	at org.apache.spark.util.Utils.copyStream(Utils.scala)
	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.writePartitionedFile(BypassMergeSortShuffleWriter.java:201)
	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:163)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)
	at org.apache.spark.scheduler.Task.run(Task.scala:123)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2019-10-30 08:09:07,580 ERROR executor.Executor: Exception in task 5.0 in stage 3.0 (TID 25)
com.esotericsoftware.kryo.KryoException: java.io.IOException: No space left on device
	at com.esotericsoftware.kryo.io.Output.flush(Output.java:188)
	at com.esotericsoftware.kryo.io.Output.require(Output.java:164)
	at com.esotericsoftware.kryo.io.Output.writeBytes(Output.java:251)
	at com.esotericsoftware.kryo.io.Output.writeBytes(Output.java:237)
	at com.esotericsoftware.kryo.serializers.DefaultArraySerializers$ByteArraySerializer.write(DefaultArraySerializers.java:49)
	at com.esotericsoftware.kryo.serializers.DefaultArraySerializers$ByteArraySerializer.write(DefaultArraySerializers.java:38)
	at com.esotericsoftware.kryo.Kryo.writeClassAndObject(Kryo.java:651)
	at org.apache.spark.serializer.KryoSerializationStream.writeObject(KryoSerializer.scala:245)
	at org.apache.spark.serializer.SerializationStream.writeValue(Serializer.scala:134)
	at org.apache.spark.storage.DiskBlockObjectWriter.write(DiskBlockObjectWriter.scala:241)
	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:151)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)
	at org.apache.spark.scheduler.Task.run(Task.scala:123)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.io.IOException: No space left on device
	at java.io.FileOutputStream.writeBytes(Native Method)
	at java.io.FileOutputStream.write(FileOutputStream.java:326)
	at org.apache.spark.storage.TimeTrackingOutputStream.write(TimeTrackingOutputStream.java:58)
	at java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:82)
	at java.io.BufferedOutputStream.flush(BufferedOutputStream.java:140)
	at java.io.FilterOutputStream.flush(FilterOutputStream.java:140)
	at com.ning.compress.lzf.LZFOutputStream.flush(LZFOutputStream.java:202)
	at com.esotericsoftware.kryo.io.Output.flush(Output.java:186)
	... 19 more
2019-10-30 08:09:07,580 ERROR executor.Executor: Exception in task 2.0 in stage 3.0 (TID 22)
com.esotericsoftware.kryo.KryoException: java.io.IOException: No space left on device
	at com.esotericsoftware.kryo.io.Output.flush(Output.java:188)
	at com.esotericsoftware.kryo.io.Output.require(Output.java:164)
	at com.esotericsoftware.kryo.io.Output.writeBytes(Output.java:251)
	at com.esotericsoftware.kryo.io.Output.writeBytes(Output.java:237)
	at com.esotericsoftware.kryo.serializers.DefaultArraySerializers$ByteArraySerializer.write(DefaultArraySerializers.java:49)
	at com.esotericsoftware.kryo.serializers.DefaultArraySerializers$ByteArraySerializer.write(DefaultArraySerializers.java:38)
	at com.esotericsoftware.kryo.Kryo.writeClassAndObject(Kryo.java:651)
	at org.apache.spark.serializer.KryoSerializationStream.writeObject(KryoSerializer.scala:245)
	at org.apache.spark.serializer.SerializationStream.writeValue(Serializer.scala:134)
	at org.apache.spark.storage.DiskBlockObjectWriter.write(DiskBlockObjectWriter.scala:241)
	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:151)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)
	at org.apache.spark.scheduler.Task.run(Task.scala:123)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.io.IOException: No space left on device
	at java.io.FileOutputStream.writeBytes(Native Method)
	at java.io.FileOutputStream.write(FileOutputStream.java:326)
	at org.apache.spark.storage.TimeTrackingOutputStream.write(TimeTrackingOutputStream.java:58)
	at java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:82)
	at java.io.BufferedOutputStream.flush(BufferedOutputStream.java:140)
	at java.io.FilterOutputStream.flush(FilterOutputStream.java:140)
	at com.ning.compress.lzf.LZFOutputStream.flush(LZFOutputStream.java:202)
	at com.esotericsoftware.kryo.io.Output.flush(Output.java:186)
	... 19 more
2019-10-30 08:09:07,580 ERROR executor.Executor: Exception in task 8.0 in stage 3.0 (TID 28)
java.io.IOException: No space left on device
	at sun.nio.ch.FileDispatcherImpl.write0(Native Method)
	at sun.nio.ch.FileDispatcherImpl.write(FileDispatcherImpl.java:60)
	at sun.nio.ch.IOUtil.writeFromNativeBuffer(IOUtil.java:93)
	at sun.nio.ch.IOUtil.write(IOUtil.java:51)
	at sun.nio.ch.FileChannelImpl.write(FileChannelImpl.java:211)
	at sun.nio.ch.FileChannelImpl.transferToTrustedChannel(FileChannelImpl.java:516)
	at sun.nio.ch.FileChannelImpl.transferTo(FileChannelImpl.java:609)
	at org.apache.spark.util.Utils$.copyFileStreamNIO(Utils.scala:389)
	at org.apache.spark.util.Utils$$anonfun$copyStream$1.apply$mcJ$sp(Utils.scala:354)
	at org.apache.spark.util.Utils$$anonfun$copyStream$1.apply(Utils.scala:348)
	at org.apache.spark.util.Utils$$anonfun$copyStream$1.apply(Utils.scala:348)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.util.Utils$.copyStream(Utils.scala:369)
	at org.apache.spark.util.Utils.copyStream(Utils.scala)
	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.writePartitionedFile(BypassMergeSortShuffleWriter.java:201)
	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:163)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)
	at org.apache.spark.scheduler.Task.run(Task.scala:123)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2019-10-30 08:09:07,578 ERROR executor.Executor: Exception in task 9.0 in stage 3.0 (TID 29)
com.esotericsoftware.kryo.KryoException: java.io.IOException: No space left on device
	at com.esotericsoftware.kryo.io.Output.flush(Output.java:188)
	at com.esotericsoftware.kryo.io.Output.require(Output.java:164)
	at com.esotericsoftware.kryo.io.Output.writeBytes(Output.java:251)
	at com.esotericsoftware.kryo.io.Output.writeBytes(Output.java:237)
	at com.esotericsoftware.kryo.serializers.DefaultArraySerializers$ByteArraySerializer.write(DefaultArraySerializers.java:49)
	at com.esotericsoftware.kryo.serializers.DefaultArraySerializers$ByteArraySerializer.write(DefaultArraySerializers.java:38)
	at com.esotericsoftware.kryo.Kryo.writeClassAndObject(Kryo.java:651)
	at org.apache.spark.serializer.KryoSerializationStream.writeObject(KryoSerializer.scala:245)
	at org.apache.spark.serializer.SerializationStream.writeKey(Serializer.scala:132)
	at org.apache.spark.storage.DiskBlockObjectWriter.write(DiskBlockObjectWriter.scala:240)
	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:151)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)
	at org.apache.spark.scheduler.Task.run(Task.scala:123)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.io.IOException: No space left on device
	at java.io.FileOutputStream.writeBytes(Native Method)
	at java.io.FileOutputStream.write(FileOutputStream.java:326)
	at org.apache.spark.storage.TimeTrackingOutputStream.write(TimeTrackingOutputStream.java:58)
	at java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:82)
	at java.io.BufferedOutputStream.flush(BufferedOutputStream.java:140)
	at java.io.FilterOutputStream.flush(FilterOutputStream.java:140)
	at com.ning.compress.lzf.LZFOutputStream.flush(LZFOutputStream.java:202)
	at com.esotericsoftware.kryo.io.Output.flush(Output.java:186)
	... 19 more
2019-10-30 08:09:07,578 ERROR executor.Executor: Exception in task 7.0 in stage 3.0 (TID 27)
java.io.IOException: No space left on device
	at sun.nio.ch.FileDispatcherImpl.write0(Native Method)
	at sun.nio.ch.FileDispatcherImpl.write(FileDispatcherImpl.java:60)
	at sun.nio.ch.IOUtil.writeFromNativeBuffer(IOUtil.java:93)
	at sun.nio.ch.IOUtil.write(IOUtil.java:51)
	at sun.nio.ch.FileChannelImpl.write(FileChannelImpl.java:211)
	at sun.nio.ch.FileChannelImpl.transferToTrustedChannel(FileChannelImpl.java:516)
	at sun.nio.ch.FileChannelImpl.transferTo(FileChannelImpl.java:609)
	at org.apache.spark.util.Utils$.copyFileStreamNIO(Utils.scala:389)
	at org.apache.spark.util.Utils$$anonfun$copyStream$1.apply$mcJ$sp(Utils.scala:354)
	at org.apache.spark.util.Utils$$anonfun$copyStream$1.apply(Utils.scala:348)
	at org.apache.spark.util.Utils$$anonfun$copyStream$1.apply(Utils.scala:348)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.util.Utils$.copyStream(Utils.scala:369)
	at org.apache.spark.util.Utils.copyStream(Utils.scala)
	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.writePartitionedFile(BypassMergeSortShuffleWriter.java:201)
	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:163)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)
	at org.apache.spark.scheduler.Task.run(Task.scala:123)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2019-10-30 08:09:07,571 ERROR executor.Executor: Exception in task 3.0 in stage 3.0 (TID 23)
java.io.IOException: No space left on device
	at sun.nio.ch.FileDispatcherImpl.write0(Native Method)
	at sun.nio.ch.FileDispatcherImpl.write(FileDispatcherImpl.java:60)
	at sun.nio.ch.IOUtil.writeFromNativeBuffer(IOUtil.java:93)
	at sun.nio.ch.IOUtil.write(IOUtil.java:51)
	at sun.nio.ch.FileChannelImpl.write(FileChannelImpl.java:211)
	at sun.nio.ch.FileChannelImpl.transferToTrustedChannel(FileChannelImpl.java:516)
	at sun.nio.ch.FileChannelImpl.transferTo(FileChannelImpl.java:609)
	at org.apache.spark.util.Utils$.copyFileStreamNIO(Utils.scala:389)
	at org.apache.spark.util.Utils$$anonfun$copyStream$1.apply$mcJ$sp(Utils.scala:354)
	at org.apache.spark.util.Utils$$anonfun$copyStream$1.apply(Utils.scala:348)
	at org.apache.spark.util.Utils$$anonfun$copyStream$1.apply(Utils.scala:348)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.util.Utils$.copyStream(Utils.scala:369)
	at org.apache.spark.util.Utils.copyStream(Utils.scala)
	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.writePartitionedFile(BypassMergeSortShuffleWriter.java:201)
	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:163)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)
	at org.apache.spark.scheduler.Task.run(Task.scala:123)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2019-10-30 08:09:07,592 ERROR executor.Executor: Exception in task 0.0 in stage 3.0 (TID 20)
java.io.IOException: No space left on device
	at sun.nio.ch.FileDispatcherImpl.write0(Native Method)
	at sun.nio.ch.FileDispatcherImpl.write(FileDispatcherImpl.java:60)
	at sun.nio.ch.IOUtil.writeFromNativeBuffer(IOUtil.java:93)
	at sun.nio.ch.IOUtil.write(IOUtil.java:51)
	at sun.nio.ch.FileChannelImpl.write(FileChannelImpl.java:211)
	at sun.nio.ch.FileChannelImpl.transferToTrustedChannel(FileChannelImpl.java:516)
	at sun.nio.ch.FileChannelImpl.transferTo(FileChannelImpl.java:609)
	at org.apache.spark.util.Utils$.copyFileStreamNIO(Utils.scala:389)
	at org.apache.spark.util.Utils$$anonfun$copyStream$1.apply$mcJ$sp(Utils.scala:354)
	at org.apache.spark.util.Utils$$anonfun$copyStream$1.apply(Utils.scala:348)
	at org.apache.spark.util.Utils$$anonfun$copyStream$1.apply(Utils.scala:348)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.util.Utils$.copyStream(Utils.scala:369)
	at org.apache.spark.util.Utils.copyStream(Utils.scala)
	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.writePartitionedFile(BypassMergeSortShuffleWriter.java:201)
	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:163)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)
	at org.apache.spark.scheduler.Task.run(Task.scala:123)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2019-10-30 08:09:07,746 WARN scheduler.TaskSetManager: Lost task 7.0 in stage 3.0 (TID 27, localhost, executor driver): java.io.IOException: No space left on device
	at sun.nio.ch.FileDispatcherImpl.write0(Native Method)
	at sun.nio.ch.FileDispatcherImpl.write(FileDispatcherImpl.java:60)
	at sun.nio.ch.IOUtil.writeFromNativeBuffer(IOUtil.java:93)
	at sun.nio.ch.IOUtil.write(IOUtil.java:51)
	at sun.nio.ch.FileChannelImpl.write(FileChannelImpl.java:211)
	at sun.nio.ch.FileChannelImpl.transferToTrustedChannel(FileChannelImpl.java:516)
	at sun.nio.ch.FileChannelImpl.transferTo(FileChannelImpl.java:609)
	at org.apache.spark.util.Utils$.copyFileStreamNIO(Utils.scala:389)
	at org.apache.spark.util.Utils$$anonfun$copyStream$1.apply$mcJ$sp(Utils.scala:354)
	at org.apache.spark.util.Utils$$anonfun$copyStream$1.apply(Utils.scala:348)
	at org.apache.spark.util.Utils$$anonfun$copyStream$1.apply(Utils.scala:348)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.util.Utils$.copyStream(Utils.scala:369)
	at org.apache.spark.util.Utils.copyStream(Utils.scala)
	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.writePartitionedFile(BypassMergeSortShuffleWriter.java:201)
	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:163)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)
	at org.apache.spark.scheduler.Task.run(Task.scala:123)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

2019-10-30 08:09:07,748 ERROR scheduler.TaskSetManager: Task 7 in stage 3.0 failed 1 times; aborting job
2019-10-30 08:09:07,756 WARN scheduler.TaskSetManager: Lost task 5.0 in stage 3.0 (TID 25, localhost, executor driver): com.esotericsoftware.kryo.KryoException: java.io.IOException: No space left on device
	at com.esotericsoftware.kryo.io.Output.flush(Output.java:188)
	at com.esotericsoftware.kryo.io.Output.require(Output.java:164)
	at com.esotericsoftware.kryo.io.Output.writeBytes(Output.java:251)
	at com.esotericsoftware.kryo.io.Output.writeBytes(Output.java:237)
	at com.esotericsoftware.kryo.serializers.DefaultArraySerializers$ByteArraySerializer.write(DefaultArraySerializers.java:49)
	at com.esotericsoftware.kryo.serializers.DefaultArraySerializers$ByteArraySerializer.write(DefaultArraySerializers.java:38)
	at com.esotericsoftware.kryo.Kryo.writeClassAndObject(Kryo.java:651)
	at org.apache.spark.serializer.KryoSerializationStream.writeObject(KryoSerializer.scala:245)
	at org.apache.spark.serializer.SerializationStream.writeValue(Serializer.scala:134)
	at org.apache.spark.storage.DiskBlockObjectWriter.write(DiskBlockObjectWriter.scala:241)
	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:151)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)
	at org.apache.spark.scheduler.Task.run(Task.scala:123)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.io.IOException: No space left on device
	at java.io.FileOutputStream.writeBytes(Native Method)
	at java.io.FileOutputStream.write(FileOutputStream.java:326)
	at org.apache.spark.storage.TimeTrackingOutputStream.write(TimeTrackingOutputStream.java:58)
	at java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:82)
	at java.io.BufferedOutputStream.flush(BufferedOutputStream.java:140)
	at java.io.FilterOutputStream.flush(FilterOutputStream.java:140)
	at com.ning.compress.lzf.LZFOutputStream.flush(LZFOutputStream.java:202)
	at com.esotericsoftware.kryo.io.Output.flush(Output.java:186)
	... 19 more

2019-10-30 08:09:07,792 ERROR io.SparkHadoopWriter: Aborting job job_20191030080901_0004.
org.apache.spark.SparkException: Job aborted due to stage failure: Task 7 in stage 3.0 failed 1 times, most recent failure: Lost task 7.0 in stage 3.0 (TID 27, localhost, executor driver): java.io.IOException: No space left on device
	at sun.nio.ch.FileDispatcherImpl.write0(Native Method)
	at sun.nio.ch.FileDispatcherImpl.write(FileDispatcherImpl.java:60)
	at sun.nio.ch.IOUtil.writeFromNativeBuffer(IOUtil.java:93)
	at sun.nio.ch.IOUtil.write(IOUtil.java:51)
	at sun.nio.ch.FileChannelImpl.write(FileChannelImpl.java:211)
	at sun.nio.ch.FileChannelImpl.transferToTrustedChannel(FileChannelImpl.java:516)
	at sun.nio.ch.FileChannelImpl.transferTo(FileChannelImpl.java:609)
	at org.apache.spark.util.Utils$.copyFileStreamNIO(Utils.scala:389)
	at org.apache.spark.util.Utils$$anonfun$copyStream$1.apply$mcJ$sp(Utils.scala:354)
	at org.apache.spark.util.Utils$$anonfun$copyStream$1.apply(Utils.scala:348)
	at org.apache.spark.util.Utils$$anonfun$copyStream$1.apply(Utils.scala:348)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.util.Utils$.copyStream(Utils.scala:369)
	at org.apache.spark.util.Utils.copyStream(Utils.scala)
	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.writePartitionedFile(BypassMergeSortShuffleWriter.java:201)
	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:163)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)
	at org.apache.spark.scheduler.Task.run(Task.scala:123)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1889)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1876)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1876)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)
	at scala.Option.foreach(Option.scala:257)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2110)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2059)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2048)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2114)
	at org.apache.spark.internal.io.SparkHadoopWriter$.write(SparkHadoopWriter.scala:78)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsNewAPIHadoopDataset$1.apply$mcV$sp(PairRDDFunctions.scala:1083)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsNewAPIHadoopDataset$1.apply(PairRDDFunctions.scala:1081)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsNewAPIHadoopDataset$1.apply(PairRDDFunctions.scala:1081)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:363)
	at org.apache.spark.rdd.PairRDDFunctions.saveAsNewAPIHadoopDataset(PairRDDFunctions.scala:1081)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsNewAPIHadoopFile$2.apply$mcV$sp(PairRDDFunctions.scala:1000)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsNewAPIHadoopFile$2.apply(PairRDDFunctions.scala:991)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsNewAPIHadoopFile$2.apply(PairRDDFunctions.scala:991)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:363)
	at org.apache.spark.rdd.PairRDDFunctions.saveAsNewAPIHadoopFile(PairRDDFunctions.scala:991)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsNewAPIHadoopFile$1.apply$mcV$sp(PairRDDFunctions.scala:979)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsNewAPIHadoopFile$1.apply(PairRDDFunctions.scala:979)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsNewAPIHadoopFile$1.apply(PairRDDFunctions.scala:979)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:363)
	at org.apache.spark.rdd.PairRDDFunctions.saveAsNewAPIHadoopFile(PairRDDFunctions.scala:978)
	at src.main.scala.terasortApp$.main(terasortApp.scala:54)
	at src.main.scala.terasortApp.main(terasortApp.scala)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.spark.deploy.JavaMainApplication.start(SparkApplication.scala:52)
	at org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:845)
	at org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:161)
	at org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:184)
	at org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:86)
	at org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:920)
	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:929)
	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
Caused by: java.io.IOException: No space left on device
	at sun.nio.ch.FileDispatcherImpl.write0(Native Method)
	at sun.nio.ch.FileDispatcherImpl.write(FileDispatcherImpl.java:60)
	at sun.nio.ch.IOUtil.writeFromNativeBuffer(IOUtil.java:93)
	at sun.nio.ch.IOUtil.write(IOUtil.java:51)
	at sun.nio.ch.FileChannelImpl.write(FileChannelImpl.java:211)
	at sun.nio.ch.FileChannelImpl.transferToTrustedChannel(FileChannelImpl.java:516)
	at sun.nio.ch.FileChannelImpl.transferTo(FileChannelImpl.java:609)
	at org.apache.spark.util.Utils$.copyFileStreamNIO(Utils.scala:389)
	at org.apache.spark.util.Utils$$anonfun$copyStream$1.apply$mcJ$sp(Utils.scala:354)
	at org.apache.spark.util.Utils$$anonfun$copyStream$1.apply(Utils.scala:348)
	at org.apache.spark.util.Utils$$anonfun$copyStream$1.apply(Utils.scala:348)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.util.Utils$.copyStream(Utils.scala:369)
	at org.apache.spark.util.Utils.copyStream(Utils.scala)
	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.writePartitionedFile(BypassMergeSortShuffleWriter.java:201)
	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:163)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)
	at org.apache.spark.scheduler.Task.run(Task.scala:123)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2019-10-30 08:09:07,809 WARN scheduler.TaskSetManager: Lost task 6.0 in stage 3.0 (TID 26, localhost, executor driver): TaskKilled (Stage cancelled)
Exception in thread "main" org.apache.spark.SparkException: Job aborted.
	at org.apache.spark.internal.io.SparkHadoopWriter$.write(SparkHadoopWriter.scala:100)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsNewAPIHadoopDataset$1.apply$mcV$sp(PairRDDFunctions.scala:1083)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsNewAPIHadoopDataset$1.apply(PairRDDFunctions.scala:1081)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsNewAPIHadoopDataset$1.apply(PairRDDFunctions.scala:1081)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:363)
	at org.apache.spark.rdd.PairRDDFunctions.saveAsNewAPIHadoopDataset(PairRDDFunctions.scala:1081)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsNewAPIHadoopFile$2.apply$mcV$sp(PairRDDFunctions.scala:1000)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsNewAPIHadoopFile$2.apply(PairRDDFunctions.scala:991)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsNewAPIHadoopFile$2.apply(PairRDDFunctions.scala:991)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:363)
	at org.apache.spark.rdd.PairRDDFunctions.saveAsNewAPIHadoopFile(PairRDDFunctions.scala:991)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsNewAPIHadoopFile$1.apply$mcV$sp(PairRDDFunctions.scala:979)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsNewAPIHadoopFile$1.apply(PairRDDFunctions.scala:979)
	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsNewAPIHadoopFile$1.apply(PairRDDFunctions.scala:979)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:363)
	at org.apache.spark.rdd.PairRDDFunctions.saveAsNewAPIHadoopFile(PairRDDFunctions.scala:978)
	at src.main.scala.terasortApp$.main(terasortApp.scala:54)
	at src.main.scala.terasortApp.main(terasortApp.scala)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.spark.deploy.JavaMainApplication.start(SparkApplication.scala:52)
	at org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:845)
	at org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:161)
	at org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:184)
	at org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:86)
	at org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:920)
	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:929)
	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
Caused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 7 in stage 3.0 failed 1 times, most recent failure: Lost task 7.0 in stage 3.0 (TID 27, localhost, executor driver): java.io.IOException: No space left on device
	at sun.nio.ch.FileDispatcherImpl.write0(Native Method)
	at sun.nio.ch.FileDispatcherImpl.write(FileDispatcherImpl.java:60)
	at sun.nio.ch.IOUtil.writeFromNativeBuffer(IOUtil.java:93)
	at sun.nio.ch.IOUtil.write(IOUtil.java:51)
	at sun.nio.ch.FileChannelImpl.write(FileChannelImpl.java:211)
	at sun.nio.ch.FileChannelImpl.transferToTrustedChannel(FileChannelImpl.java:516)
	at sun.nio.ch.FileChannelImpl.transferTo(FileChannelImpl.java:609)
	at org.apache.spark.util.Utils$.copyFileStreamNIO(Utils.scala:389)
	at org.apache.spark.util.Utils$$anonfun$copyStream$1.apply$mcJ$sp(Utils.scala:354)
	at org.apache.spark.util.Utils$$anonfun$copyStream$1.apply(Utils.scala:348)
	at org.apache.spark.util.Utils$$anonfun$copyStream$1.apply(Utils.scala:348)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.util.Utils$.copyStream(Utils.scala:369)
	at org.apache.spark.util.Utils.copyStream(Utils.scala)
	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.writePartitionedFile(BypassMergeSortShuffleWriter.java:201)
	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:163)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)
	at org.apache.spark.scheduler.Task.run(Task.scala:123)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1889)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1876)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1876)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)
	at scala.Option.foreach(Option.scala:257)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2110)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2059)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2048)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2114)
	at org.apache.spark.internal.io.SparkHadoopWriter$.write(SparkHadoopWriter.scala:78)
	... 35 more
Caused by: java.io.IOException: No space left on device
	at sun.nio.ch.FileDispatcherImpl.write0(Native Method)
	at sun.nio.ch.FileDispatcherImpl.write(FileDispatcherImpl.java:60)
	at sun.nio.ch.IOUtil.writeFromNativeBuffer(IOUtil.java:93)
	at sun.nio.ch.IOUtil.write(IOUtil.java:51)
	at sun.nio.ch.FileChannelImpl.write(FileChannelImpl.java:211)
	at sun.nio.ch.FileChannelImpl.transferToTrustedChannel(FileChannelImpl.java:516)
	at sun.nio.ch.FileChannelImpl.transferTo(FileChannelImpl.java:609)
	at org.apache.spark.util.Utils$.copyFileStreamNIO(Utils.scala:389)
	at org.apache.spark.util.Utils$$anonfun$copyStream$1.apply$mcJ$sp(Utils.scala:354)
	at org.apache.spark.util.Utils$$anonfun$copyStream$1.apply(Utils.scala:348)
	at org.apache.spark.util.Utils$$anonfun$copyStream$1.apply(Utils.scala:348)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.util.Utils$.copyStream(Utils.scala:369)
	at org.apache.spark.util.Utils.copyStream(Utils.scala)
	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.writePartitionedFile(BypassMergeSortShuffleWriter.java:201)
	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:163)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)
	at org.apache.spark.scheduler.Task.run(Task.scala:123)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2019-10-30 08:09:07,908 INFO server.AbstractConnector: Stopped Spark@51745f40{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
application termination...
	Command being timed: "Terasort/bin/run.sh"
	User time (seconds): 127.01
	System time (seconds): 25.17
	Percent of CPU this job got: 231%
	Elapsed (wall clock) time (h:mm:ss or m:ss): 1:05.82
	Average shared text size (kbytes): 0
	Average unshared data size (kbytes): 0
	Average stack size (kbytes): 0
	Average total size (kbytes): 0
	Maximum resident set size (kbytes): 2557792
	Average resident set size (kbytes): 0
	Major (requiring I/O) page faults: 229
	Minor (reclaiming a frame) page faults: 197984
	Voluntary context switches: 81795
	Involuntary context switches: 53550
	Swaps: 0
	File system inputs: 468800
	File system outputs: 3367992
	Socket messages sent: 0
	Socket messages received: 0
	Signals delivered: 0
	Page size (bytes): 4096
	Exit status: 0
+ exit
[46177.394063] flag is set to print stats 2
[46177.394069] cache-hits 0 cache-miss 0 buff-hits 0 buff-miss 0 migrated 0 cache-del 0 buff-del 0 
[46177.394070] flag set to clear count 0
[46177.394075] flag is set to collect hetero allocate  9 
[46188.059221] audit: type=1400 audit(1572444381.495:4313): apparmor="DENIED" operation="open" profile="/usr/sbin/mysqld" name="/proc/17956/status" pid=17956 comm="mysqld" requested_mask="r" denied_mask="r" fsuid=114 ouid=114
[46188.059336] audit: type=1400 audit(1572444381.495:4314): apparmor="DENIED" operation="open" profile="/usr/sbin/mysqld" name="/sys/devices/system/node/" pid=17956 comm="mysqld" requested_mask="r" denied_mask="r" fsuid=114 ouid=0
[46188.059497] audit: type=1400 audit(1572444381.495:4315): apparmor="DENIED" operation="open" profile="/usr/sbin/mysqld" name="/proc/17956/status" pid=17956 comm="mysqld" requested_mask="r" denied_mask="r" fsuid=114 ouid=114
[46219.562223] audit: type=1400 audit(1572444413.000:4316): apparmor="DENIED" operation="open" profile="/usr/sbin/mysqld" name="/proc/18914/status" pid=18914 comm="mysqld" requested_mask="r" denied_mask="r" fsuid=114 ouid=114
[46219.562232] audit: type=1400 audit(1572444413.000:4317): apparmor="DENIED" operation="open" profile="/usr/sbin/mysqld" name="/sys/devices/system/node/" pid=18914 comm="mysqld" requested_mask="r" denied_mask="r" fsuid=114 ouid=0
[46219.562330] audit: type=1400 audit(1572444413.000:4318): apparmor="DENIED" operation="open" profile="/usr/sbin/mysqld" name="/proc/18914/status" pid=18914 comm="mysqld" requested_mask="r" denied_mask="r" fsuid=114 ouid=114
[46252.569159] audit: type=1400 audit(1572444446.008:4319): apparmor="DENIED" operation="open" profile="/usr/sbin/mysqld" name="/proc/20327/status" pid=20327 comm="mysqld" requested_mask="r" denied_mask="r" fsuid=114 ouid=114
[46252.569182] audit: type=1400 audit(1572444446.008:4320): apparmor="DENIED" operation="open" profile="/usr/sbin/mysqld" name="/sys/devices/system/node/" pid=20327 comm="mysqld" requested_mask="r" denied_mask="r" fsuid=114 ouid=0
[46252.572323] audit: type=1400 audit(1572444446.008:4321): apparmor="DENIED" operation="open" profile="/usr/sbin/mysqld" name="/proc/20327/status" pid=20327 comm="mysqld" requested_mask="r" denied_mask="r" fsuid=114 ouid=114
[46285.570987] audit: type=1400 audit(1572444479.013:4322): apparmor="DENIED" operation="open" profile="/usr/sbin/mysqld" name="/proc/20931/status" pid=20931 comm="mysqld" requested_mask="r" denied_mask="r" fsuid=114 ouid=114
[46285.571733] audit: type=1400 audit(1572444479.013:4323): apparmor="DENIED" operation="open" profile="/usr/sbin/mysqld" name="/sys/devices/system/node/" pid=20931 comm="mysqld" requested_mask="r" denied_mask="r" fsuid=114 ouid=0
[46285.571749] audit: type=1400 audit(1572444479.013:4324): apparmor="DENIED" operation="open" profile="/usr/sbin/mysqld" name="/proc/20931/status" pid=20931 comm="mysqld" requested_mask="r" denied_mask="r" fsuid=114 ouid=114
[46306.504488] drop_caches: sh (21418): drop_caches: 3
[46318.830419] audit: type=1400 audit(1572444512.270:4325): apparmor="DENIED" operation="open" profile="/usr/sbin/mysqld" name="/proc/21698/status" pid=21698 comm="mysqld" requested_mask="r" denied_mask="r" fsuid=114 ouid=114
[46318.831041] audit: type=1400 audit(1572444512.270:4326): apparmor="DENIED" operation="open" profile="/usr/sbin/mysqld" name="/sys/devices/system/node/" pid=21698 comm="mysqld" requested_mask="r" denied_mask="r" fsuid=114 ouid=0
[46318.831049] audit: type=1400 audit(1572444512.270:4327): apparmor="DENIED" operation="open" profile="/usr/sbin/mysqld" name="/proc/21698/status" pid=21698 comm="mysqld" requested_mask="r" denied_mask="r" fsuid=114 ouid=114
[46350.831826] audit: type=1400 audit(1572444544.271:4328): apparmor="DENIED" operation="open" profile="/usr/sbin/mysqld" name="/proc/21922/status" pid=21922 comm="mysqld" requested_mask="r" denied_mask="r" fsuid=114 ouid=114
[46350.831937] audit: type=1400 audit(1572444544.271:4329): apparmor="DENIED" operation="open" profile="/usr/sbin/mysqld" name="/sys/devices/system/node/" pid=21922 comm="mysqld" requested_mask="r" denied_mask="r" fsuid=114 ouid=0
[46350.832072] audit: type=1400 audit(1572444544.271:4330): apparmor="DENIED" operation="open" profile="/usr/sbin/mysqld" name="/proc/21922/status" pid=21922 comm="mysqld" requested_mask="r" denied_mask="r" fsuid=114 ouid=114
